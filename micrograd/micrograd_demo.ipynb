{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ccc1aa-d25b-4503-a853-6c7bc278c611",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'micrograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmicrograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLP\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mr\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'micrograd'"
     ]
    }
   ],
   "source": [
    "from nn import MLP\n",
    "import random as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9834207-7ac5-4d7e-ba41-2ea874d8fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a tiny dataset\n",
    "size = 8192\n",
    "x = [[r.uniform(-1,1) for _ in range(4)] for _ in range(size)]\n",
    "y = []\n",
    "for features in x:\n",
    "    # Create a pattern: y = 2*x1 - 0.5*x2 + 3*x3 - x4 + noise\n",
    "    y_value = (\n",
    "        2 * features[0]\n",
    "        - 0.5 * features[1]\n",
    "        + 3 * features[2]\n",
    "        - features[3]\n",
    "        + r.uniform(-0.1, 0.1)  # Add some noise\n",
    "    )\n",
    "    y.append(y_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97655995-df60-409a-917f-254303511c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.985702240884134, grad=0.0),\n",
       " Value(data=0.9055957120168918, grad=0.0),\n",
       " Value(data=0.365524002542593, grad=0.0),\n",
       " Value(data=0.17442747462656394, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.22010930200012502, grad=0.0),\n",
       " Value(data=0.8238840604535325, grad=0.0),\n",
       " Value(data=0.5740417420229842, grad=0.0),\n",
       " Value(data=0.6591262029582623, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=0.8458949266933498, grad=0.0),\n",
       " Value(data=-0.8380457030889297, grad=0.0),\n",
       " Value(data=0.15417534878301242, grad=0.0),\n",
       " Value(data=-0.8042162152019321, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.8190995978058142, grad=0.0),\n",
       " Value(data=0.4405593387434992, grad=0.0),\n",
       " Value(data=0.0886371625445701, grad=0.0),\n",
       " Value(data=-0.7966443911791146, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.18020173213800605, grad=0.0),\n",
       " Value(data=-0.1752621671020056, grad=0.0),\n",
       " Value(data=-0.8664593399070553, grad=0.0),\n",
       " Value(data=-0.5908780140845074, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.2701279854335419, grad=0.0),\n",
       " Value(data=0.32235968877648413, grad=0.0),\n",
       " Value(data=0.8558255402952311, grad=0.0),\n",
       " Value(data=-0.6239594272620661, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=0.02373974106801846, grad=0.0),\n",
       " Value(data=0.8557555338777987, grad=0.0),\n",
       " Value(data=-0.24988827471525332, grad=0.0),\n",
       " Value(data=0.12789726574527793, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.5638651888477204, grad=0.0),\n",
       " Value(data=-0.045021932582388624, grad=0.0),\n",
       " Value(data=-0.9986784137326858, grad=0.0),\n",
       " Value(data=0.2733461597249778, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.49582029202414124, grad=0.0),\n",
       " Value(data=0.31524951440666693, grad=0.0),\n",
       " Value(data=-0.6042846924371024, grad=0.0),\n",
       " Value(data=0.34370524173290584, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.7050121861120573, grad=0.0),\n",
       " Value(data=-0.08071828035571271, grad=0.0),\n",
       " Value(data=0.3181597742693518, grad=0.0),\n",
       " Value(data=-0.32173692762684825, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.33666159577287735, grad=0.0),\n",
       " Value(data=0.18322433985651387, grad=0.0),\n",
       " Value(data=0.573595364799538, grad=0.0),\n",
       " Value(data=-0.85567497214325, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.5423016019793507, grad=0.0),\n",
       " Value(data=0.4254796522862625, grad=0.0),\n",
       " Value(data=-0.38325911809829893, grad=0.0),\n",
       " Value(data=0.3799962594641939, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.38974105783549184, grad=0.0),\n",
       " Value(data=-0.20749091285186583, grad=0.0),\n",
       " Value(data=-0.7434410501706556, grad=0.0),\n",
       " Value(data=0.21234914636074098, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.5224635211472495, grad=0.0),\n",
       " Value(data=0.9169927870640557, grad=0.0),\n",
       " Value(data=-0.5785117752279736, grad=0.0),\n",
       " Value(data=0.8311764056394739, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=0.9909937845584478, grad=0.0),\n",
       " Value(data=0.08691765046309419, grad=0.0),\n",
       " Value(data=-0.1593532879906363, grad=0.0),\n",
       " Value(data=0.5496238007461223, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.9965835475126721, grad=0.0),\n",
       " Value(data=0.20077420930857848, grad=0.0),\n",
       " Value(data=0.9220498639087482, grad=0.0),\n",
       " Value(data=0.2141397935303606, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=0.980003554482114, grad=0.0),\n",
       " Value(data=-0.4294938216474333, grad=0.0),\n",
       " Value(data=0.38404171827812084, grad=0.0),\n",
       " Value(data=0.7018854495219329, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.13307587563312784, grad=0.0),\n",
       " Value(data=0.5779056897750092, grad=0.0),\n",
       " Value(data=0.9016142020637259, grad=0.0),\n",
       " Value(data=-0.7665640493715808, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.09312059620715396, grad=0.0),\n",
       " Value(data=0.3366575897369426, grad=0.0),\n",
       " Value(data=-0.10969224381065201, grad=0.0),\n",
       " Value(data=0.8670709822249478, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=0.2981770856564596, grad=0.0),\n",
       " Value(data=-0.4366321478250983, grad=0.0),\n",
       " Value(data=0.38971208521703526, grad=0.0),\n",
       " Value(data=0.8524731617050616, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=0.07707249028932295, grad=0.0),\n",
       " Value(data=-0.20895254990608247, grad=0.0),\n",
       " Value(data=0.08222897940463114, grad=0.0),\n",
       " Value(data=-0.1726342751813803, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.8974100479951097, grad=0.0),\n",
       " Value(data=-0.044589520053774256, grad=0.0),\n",
       " Value(data=0.6037603694570262, grad=0.0),\n",
       " Value(data=-0.01799353013047167, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.6235898888779847, grad=0.0),\n",
       " Value(data=-0.3837343497243837, grad=0.0),\n",
       " Value(data=0.4423628819222378, grad=0.0),\n",
       " Value(data=-0.6462972004819385, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.8180684892815631, grad=0.0),\n",
       " Value(data=0.6902063739956794, grad=0.0),\n",
       " Value(data=-0.3326052136264437, grad=0.0),\n",
       " Value(data=-0.8453170010262887, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.6357938338494593, grad=0.0),\n",
       " Value(data=-0.9239640116316026, grad=0.0),\n",
       " Value(data=0.17616026835863074, grad=0.0),\n",
       " Value(data=0.49227261506196984, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=0.4366659760276239, grad=0.0),\n",
       " Value(data=-0.0896337530504221, grad=0.0),\n",
       " Value(data=-0.5520986049680408, grad=0.0),\n",
       " Value(data=0.46056015023261265, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=0.2086746966032642, grad=0.0),\n",
       " Value(data=-0.2563145794684296, grad=0.0),\n",
       " Value(data=0.7435741964228892, grad=0.0),\n",
       " Value(data=-0.43523536548237707, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.1687831676678535, grad=0.0),\n",
       " Value(data=-0.676444017255736, grad=0.0),\n",
       " Value(data=-0.12196166151771126, grad=0.0),\n",
       " Value(data=-0.2300955833398337, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.36447007805630083, grad=0.0),\n",
       " Value(data=0.1913670515542385, grad=0.0),\n",
       " Value(data=0.06228859777172091, grad=0.0),\n",
       " Value(data=0.08845218879045458, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.6751343255887279, grad=0.0),\n",
       " Value(data=0.3971574425943518, grad=0.0),\n",
       " Value(data=0.4775435491110167, grad=0.0),\n",
       " Value(data=0.8300394616424349, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.5412633095060067, grad=0.0),\n",
       " Value(data=0.05797291081584244, grad=0.0),\n",
       " Value(data=0.10668509605294019, grad=0.0),\n",
       " Value(data=-0.7019032837050407, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=0.09109132216544613, grad=0.0),\n",
       " Value(data=0.23755156356905127, grad=0.0),\n",
       " Value(data=-0.8216013841043281, grad=0.0),\n",
       " Value(data=-0.8560791763962001, grad=0.0),\n",
       " Value(data=0.0, grad=0.0),\n",
       " Value(data=-0.08549317746657703, grad=0.0),\n",
       " Value(data=-0.9363245269453881, grad=0.0),\n",
       " Value(data=0.5408756013646154, grad=0.0),\n",
       " Value(data=0.6260197811988015, grad=0.0),\n",
       " Value(data=-0.4268799797900198, grad=0.0),\n",
       " Value(data=-0.13443633035445957, grad=0.0),\n",
       " Value(data=-0.21839596459644794, grad=0.0),\n",
       " Value(data=-0.8599809339837632, grad=0.0),\n",
       " Value(data=0.03617538555040767, grad=0.0),\n",
       " Value(data=0.1637261838787425, grad=0.0),\n",
       " Value(data=-0.9510098938284322, grad=0.0),\n",
       " Value(data=0.5857969397942149, grad=0.0),\n",
       " Value(data=-0.8289133622949623, grad=0.0),\n",
       " Value(data=0.7559765962303915, grad=0.0),\n",
       " Value(data=0.9797924229827872, grad=0.0),\n",
       " Value(data=-0.8333119655007686, grad=0.0),\n",
       " Value(data=-0.8153861441095598, grad=0.0),\n",
       " Value(data=-0.26367643626125803, grad=0.0),\n",
       " Value(data=0.016290316947585648, grad=0.0),\n",
       " Value(data=-0.6997440484144817, grad=0.0),\n",
       " Value(data=0.667998765203222, grad=0.0),\n",
       " Value(data=-0.9920018886357156, grad=0.0),\n",
       " Value(data=-0.8876833417750409, grad=0.0),\n",
       " Value(data=0.28843763268740963, grad=0.0),\n",
       " Value(data=0.7981183403787371, grad=0.0),\n",
       " Value(data=0.7310857215038657, grad=0.0),\n",
       " Value(data=0.6799314184541558, grad=0.0),\n",
       " Value(data=-0.42569864832609805, grad=0.0),\n",
       " Value(data=0.33829830311100073, grad=0.0),\n",
       " Value(data=-0.4182267292935591, grad=0.0),\n",
       " Value(data=0.9607110276103741, grad=0.0),\n",
       " Value(data=0.3751117416670944, grad=0.0),\n",
       " Value(data=0.0, grad=0.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize an MLP with 4 input neurons, hidden_dim of 32, and 1 output neuron\n",
    "model = MLP(len(x[0]), len(x[0])*8, 1)\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f53de63-4ede-45d7-a789-22601baca735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial test set loss: Value(data=114.89460601008064, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "# initial loss on test set, which will just be the final batch in the dataset\n",
    "batch_size = 16\n",
    "x_test = x[-batch_size:]\n",
    "y_test = y[-batch_size:]\n",
    "\n",
    "# running the model over the dataset\n",
    "ypred = []\n",
    "for j in range(batch_size):\n",
    "    ypred.append(model(x_test[j]))\n",
    "\n",
    "# MSE loss function\n",
    "loss_batch = [(yhat - ytrue)**2 for ytrue, yhat in zip(y_test, ypred)]\n",
    "loss = 0.0\n",
    "for k in loss_batch: # sum() doesn't work for some reason so we've gotta do it manually\n",
    "    loss = loss + k\n",
    "    \n",
    "print(f'initial test set loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356cd885-fcdc-4859-b4fd-e9c9295479a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 loss: Value(data=102.4861454754203, grad=0.0)\n",
      "step 2 loss: Value(data=82.82057175308601, grad=0.0)\n",
      "step 3 loss: Value(data=100.00733407315165, grad=0.0)\n",
      "step 4 loss: Value(data=133.98432390108005, grad=0.0)\n",
      "step 5 loss: Value(data=86.78229654540696, grad=0.0)\n",
      "step 6 loss: Value(data=106.63538063123342, grad=0.0)\n",
      "step 7 loss: Value(data=50.13581100489319, grad=0.0)\n",
      "step 8 loss: Value(data=122.55818099627948, grad=0.0)\n",
      "step 9 loss: Value(data=122.40644285430507, grad=0.0)\n",
      "step 10 loss: Value(data=118.04609630374587, grad=0.0)\n",
      "step 11 loss: Value(data=80.10721409578956, grad=0.0)\n",
      "step 12 loss: Value(data=82.60633635421839, grad=0.0)\n",
      "step 13 loss: Value(data=100.33185025800138, grad=0.0)\n",
      "step 14 loss: Value(data=74.7824293345868, grad=0.0)\n",
      "step 15 loss: Value(data=118.98993479444492, grad=0.0)\n",
      "step 16 loss: Value(data=75.42351405569543, grad=0.0)\n",
      "step 17 loss: Value(data=118.8394766275158, grad=0.0)\n",
      "step 18 loss: Value(data=81.97656808131947, grad=0.0)\n",
      "step 19 loss: Value(data=63.33664835212338, grad=0.0)\n",
      "step 20 loss: Value(data=81.1526498808469, grad=0.0)\n",
      "step 21 loss: Value(data=152.02525792818125, grad=0.0)\n",
      "step 22 loss: Value(data=68.16724898559362, grad=0.0)\n",
      "step 23 loss: Value(data=65.59735647046999, grad=0.0)\n",
      "step 24 loss: Value(data=93.80333757692618, grad=0.0)\n",
      "step 25 loss: Value(data=109.0037959399406, grad=0.0)\n",
      "step 26 loss: Value(data=74.87852513143103, grad=0.0)\n",
      "step 27 loss: Value(data=82.50999234581096, grad=0.0)\n",
      "step 28 loss: Value(data=61.63238824128938, grad=0.0)\n",
      "step 29 loss: Value(data=33.257140535995575, grad=0.0)\n",
      "step 30 loss: Value(data=38.59101641519595, grad=0.0)\n",
      "step 31 loss: Value(data=89.86440896506542, grad=0.0)\n",
      "step 32 loss: Value(data=55.06574299466431, grad=0.0)\n",
      "step 33 loss: Value(data=66.01224368403014, grad=0.0)\n",
      "step 34 loss: Value(data=56.87082883984665, grad=0.0)\n",
      "step 35 loss: Value(data=87.95918153720439, grad=0.0)\n",
      "step 36 loss: Value(data=70.60488234097458, grad=0.0)\n",
      "step 37 loss: Value(data=54.643381491629135, grad=0.0)\n",
      "step 38 loss: Value(data=73.73714738599642, grad=0.0)\n",
      "step 39 loss: Value(data=93.49863755247331, grad=0.0)\n",
      "step 40 loss: Value(data=52.895560827462674, grad=0.0)\n",
      "step 41 loss: Value(data=68.48133635870775, grad=0.0)\n",
      "step 42 loss: Value(data=47.428184371017316, grad=0.0)\n",
      "step 43 loss: Value(data=66.87172058953036, grad=0.0)\n",
      "step 44 loss: Value(data=58.35822377940387, grad=0.0)\n",
      "step 45 loss: Value(data=49.95222491944002, grad=0.0)\n",
      "step 46 loss: Value(data=69.14048113217765, grad=0.0)\n",
      "step 47 loss: Value(data=65.8480906356478, grad=0.0)\n",
      "step 48 loss: Value(data=44.07726535018294, grad=0.0)\n",
      "step 49 loss: Value(data=40.17879699866897, grad=0.0)\n",
      "step 50 loss: Value(data=43.28967231495187, grad=0.0)\n",
      "step 51 loss: Value(data=90.62176987473516, grad=0.0)\n",
      "step 52 loss: Value(data=56.01284918661923, grad=0.0)\n",
      "step 53 loss: Value(data=47.9061058489346, grad=0.0)\n",
      "step 54 loss: Value(data=64.64187103166476, grad=0.0)\n",
      "step 55 loss: Value(data=64.18212704814628, grad=0.0)\n",
      "step 56 loss: Value(data=59.213573359884485, grad=0.0)\n",
      "step 57 loss: Value(data=46.541167996104754, grad=0.0)\n",
      "step 58 loss: Value(data=42.959365895926624, grad=0.0)\n",
      "step 59 loss: Value(data=82.48389468900885, grad=0.0)\n",
      "step 60 loss: Value(data=79.40098465826323, grad=0.0)\n",
      "step 61 loss: Value(data=48.82527360256072, grad=0.0)\n",
      "step 62 loss: Value(data=27.378030808897996, grad=0.0)\n",
      "step 63 loss: Value(data=47.550576150293566, grad=0.0)\n",
      "step 64 loss: Value(data=33.73406860606481, grad=0.0)\n",
      "step 65 loss: Value(data=36.239117616758776, grad=0.0)\n",
      "step 66 loss: Value(data=44.95840078687884, grad=0.0)\n",
      "step 67 loss: Value(data=23.169409843405433, grad=0.0)\n",
      "step 68 loss: Value(data=53.19217573482918, grad=0.0)\n",
      "step 69 loss: Value(data=41.284153101940106, grad=0.0)\n",
      "step 70 loss: Value(data=25.540713530105077, grad=0.0)\n",
      "step 71 loss: Value(data=50.359854038464405, grad=0.0)\n",
      "step 72 loss: Value(data=27.79560044059614, grad=0.0)\n",
      "step 73 loss: Value(data=41.34742464395928, grad=0.0)\n",
      "step 74 loss: Value(data=46.961238407098655, grad=0.0)\n",
      "step 75 loss: Value(data=26.659925146529954, grad=0.0)\n",
      "step 76 loss: Value(data=40.0584243624406, grad=0.0)\n",
      "step 77 loss: Value(data=20.873194602032957, grad=0.0)\n",
      "step 78 loss: Value(data=30.845441202203368, grad=0.0)\n",
      "step 79 loss: Value(data=39.325910867760506, grad=0.0)\n",
      "step 80 loss: Value(data=32.70524252206556, grad=0.0)\n",
      "step 81 loss: Value(data=28.929722746745153, grad=0.0)\n",
      "step 82 loss: Value(data=35.373453114639375, grad=0.0)\n",
      "step 83 loss: Value(data=23.809082569289966, grad=0.0)\n",
      "step 84 loss: Value(data=41.05481967601342, grad=0.0)\n",
      "step 85 loss: Value(data=34.43760142304321, grad=0.0)\n",
      "step 86 loss: Value(data=31.22697079384255, grad=0.0)\n",
      "step 87 loss: Value(data=42.78969087084342, grad=0.0)\n",
      "step 88 loss: Value(data=22.55608089848148, grad=0.0)\n",
      "step 89 loss: Value(data=34.22981661663509, grad=0.0)\n",
      "step 90 loss: Value(data=28.04349197790956, grad=0.0)\n",
      "step 91 loss: Value(data=31.348058414770264, grad=0.0)\n",
      "step 92 loss: Value(data=38.069610052521575, grad=0.0)\n",
      "step 93 loss: Value(data=24.111666070908, grad=0.0)\n",
      "step 94 loss: Value(data=36.440159348507564, grad=0.0)\n",
      "step 95 loss: Value(data=38.48762775096661, grad=0.0)\n",
      "step 96 loss: Value(data=47.98205882889398, grad=0.0)\n",
      "step 97 loss: Value(data=31.495407433239475, grad=0.0)\n",
      "step 98 loss: Value(data=10.701945782549771, grad=0.0)\n",
      "step 99 loss: Value(data=31.51926221762007, grad=0.0)\n",
      "step 100 loss: Value(data=23.06840788978914, grad=0.0)\n",
      "step 101 loss: Value(data=15.931160713633076, grad=0.0)\n",
      "step 102 loss: Value(data=27.38197329559098, grad=0.0)\n",
      "step 103 loss: Value(data=25.46321232890624, grad=0.0)\n",
      "step 104 loss: Value(data=23.116508750224654, grad=0.0)\n",
      "step 105 loss: Value(data=31.603547795334926, grad=0.0)\n",
      "step 106 loss: Value(data=35.10703535725724, grad=0.0)\n",
      "step 107 loss: Value(data=25.336446132533446, grad=0.0)\n",
      "step 108 loss: Value(data=23.755351129247632, grad=0.0)\n",
      "step 109 loss: Value(data=29.85541555477408, grad=0.0)\n",
      "step 110 loss: Value(data=21.562829417245226, grad=0.0)\n",
      "step 111 loss: Value(data=20.234725190156595, grad=0.0)\n",
      "step 112 loss: Value(data=24.716757257116996, grad=0.0)\n",
      "step 113 loss: Value(data=33.78690152569779, grad=0.0)\n",
      "step 114 loss: Value(data=15.590713381657118, grad=0.0)\n",
      "step 115 loss: Value(data=17.68454039982963, grad=0.0)\n",
      "step 116 loss: Value(data=20.028392661212745, grad=0.0)\n",
      "step 117 loss: Value(data=17.997453937524032, grad=0.0)\n",
      "step 118 loss: Value(data=23.42191142654239, grad=0.0)\n",
      "step 119 loss: Value(data=32.789594322166536, grad=0.0)\n",
      "step 120 loss: Value(data=16.241026219135176, grad=0.0)\n",
      "step 121 loss: Value(data=16.741542350291315, grad=0.0)\n",
      "step 122 loss: Value(data=18.341689545837784, grad=0.0)\n",
      "step 123 loss: Value(data=29.98140822233554, grad=0.0)\n",
      "step 124 loss: Value(data=10.734443533796725, grad=0.0)\n",
      "step 125 loss: Value(data=18.347684660404884, grad=0.0)\n",
      "step 126 loss: Value(data=16.164671579428568, grad=0.0)\n",
      "step 127 loss: Value(data=18.839370083763427, grad=0.0)\n",
      "step 128 loss: Value(data=26.888206876358357, grad=0.0)\n",
      "step 129 loss: Value(data=12.144238382510002, grad=0.0)\n",
      "step 130 loss: Value(data=13.660465359969958, grad=0.0)\n",
      "step 131 loss: Value(data=10.755122826874173, grad=0.0)\n",
      "step 132 loss: Value(data=10.222966398971067, grad=0.0)\n",
      "step 133 loss: Value(data=15.448066354035491, grad=0.0)\n",
      "step 134 loss: Value(data=15.788405514642834, grad=0.0)\n",
      "step 135 loss: Value(data=15.473515609541087, grad=0.0)\n",
      "step 136 loss: Value(data=11.910425858997822, grad=0.0)\n",
      "step 137 loss: Value(data=23.208960207152323, grad=0.0)\n",
      "step 138 loss: Value(data=18.634049389782447, grad=0.0)\n",
      "step 139 loss: Value(data=12.476608358578279, grad=0.0)\n",
      "step 140 loss: Value(data=16.79379715185383, grad=0.0)\n",
      "step 141 loss: Value(data=22.33005351881919, grad=0.0)\n",
      "step 142 loss: Value(data=17.612191661611323, grad=0.0)\n",
      "step 143 loss: Value(data=16.718115768236103, grad=0.0)\n",
      "step 144 loss: Value(data=6.810460468872099, grad=0.0)\n",
      "step 145 loss: Value(data=25.893352978787554, grad=0.0)\n",
      "step 146 loss: Value(data=21.376695861296685, grad=0.0)\n",
      "step 147 loss: Value(data=11.20269777148811, grad=0.0)\n",
      "step 148 loss: Value(data=6.97545450362494, grad=0.0)\n",
      "step 149 loss: Value(data=9.06036407111859, grad=0.0)\n",
      "step 150 loss: Value(data=6.029460126162541, grad=0.0)\n",
      "step 151 loss: Value(data=15.580673934291156, grad=0.0)\n",
      "step 152 loss: Value(data=16.251396762000287, grad=0.0)\n",
      "step 153 loss: Value(data=12.818391179771224, grad=0.0)\n",
      "step 154 loss: Value(data=10.400037122872071, grad=0.0)\n",
      "step 155 loss: Value(data=10.737969700705145, grad=0.0)\n",
      "step 156 loss: Value(data=15.42898423097257, grad=0.0)\n",
      "step 157 loss: Value(data=19.0496802240026, grad=0.0)\n",
      "step 158 loss: Value(data=11.535847861181557, grad=0.0)\n",
      "step 159 loss: Value(data=5.910164340992605, grad=0.0)\n",
      "step 160 loss: Value(data=15.453793333632076, grad=0.0)\n",
      "step 161 loss: Value(data=4.052216654245439, grad=0.0)\n",
      "step 162 loss: Value(data=19.490584906467554, grad=0.0)\n",
      "step 163 loss: Value(data=18.271473285433544, grad=0.0)\n",
      "step 164 loss: Value(data=16.779075779916713, grad=0.0)\n",
      "step 165 loss: Value(data=7.763220188266058, grad=0.0)\n",
      "step 166 loss: Value(data=20.16678497704264, grad=0.0)\n",
      "step 167 loss: Value(data=6.840603542130362, grad=0.0)\n",
      "step 168 loss: Value(data=5.205026454842979, grad=0.0)\n",
      "step 169 loss: Value(data=12.289617031655421, grad=0.0)\n",
      "step 170 loss: Value(data=20.055855093392104, grad=0.0)\n",
      "step 171 loss: Value(data=14.476671096686632, grad=0.0)\n",
      "step 172 loss: Value(data=3.3525183104318472, grad=0.0)\n",
      "step 173 loss: Value(data=11.877093604370424, grad=0.0)\n",
      "step 174 loss: Value(data=5.306601575521485, grad=0.0)\n",
      "step 175 loss: Value(data=7.81115049797066, grad=0.0)\n",
      "step 176 loss: Value(data=11.602168029663488, grad=0.0)\n",
      "step 177 loss: Value(data=6.1544815124463135, grad=0.0)\n",
      "step 178 loss: Value(data=4.364267518789239, grad=0.0)\n",
      "step 179 loss: Value(data=3.248074297665635, grad=0.0)\n",
      "step 180 loss: Value(data=15.069378845515189, grad=0.0)\n",
      "step 181 loss: Value(data=12.279730445652701, grad=0.0)\n",
      "step 182 loss: Value(data=17.15818794103039, grad=0.0)\n",
      "step 183 loss: Value(data=7.9111517235108035, grad=0.0)\n",
      "step 184 loss: Value(data=8.904292844637904, grad=0.0)\n",
      "step 185 loss: Value(data=10.064965164746544, grad=0.0)\n",
      "step 186 loss: Value(data=3.4363836706975195, grad=0.0)\n",
      "step 187 loss: Value(data=2.750639968164524, grad=0.0)\n",
      "step 188 loss: Value(data=8.440069007930568, grad=0.0)\n",
      "step 189 loss: Value(data=11.556535721837154, grad=0.0)\n",
      "step 190 loss: Value(data=3.4378979244267684, grad=0.0)\n",
      "step 191 loss: Value(data=8.131579213225951, grad=0.0)\n",
      "step 192 loss: Value(data=5.470074187823158, grad=0.0)\n",
      "step 193 loss: Value(data=12.698675132994511, grad=0.0)\n",
      "step 194 loss: Value(data=9.19208152997151, grad=0.0)\n",
      "step 195 loss: Value(data=7.174681179590707, grad=0.0)\n",
      "step 196 loss: Value(data=11.891756507656801, grad=0.0)\n",
      "step 197 loss: Value(data=11.498213231235024, grad=0.0)\n",
      "step 198 loss: Value(data=3.4350879856919976, grad=0.0)\n",
      "step 199 loss: Value(data=6.069344704459018, grad=0.0)\n",
      "step 200 loss: Value(data=14.008861762471888, grad=0.0)\n",
      "step 201 loss: Value(data=10.245320079047321, grad=0.0)\n",
      "step 202 loss: Value(data=8.634797762590892, grad=0.0)\n",
      "step 203 loss: Value(data=3.7872906810581437, grad=0.0)\n",
      "step 204 loss: Value(data=11.77846978996809, grad=0.0)\n",
      "step 205 loss: Value(data=6.16022488364111, grad=0.0)\n",
      "step 206 loss: Value(data=1.1861982236020456, grad=0.0)\n",
      "step 207 loss: Value(data=7.6146577816128085, grad=0.0)\n",
      "step 208 loss: Value(data=7.070503073278866, grad=0.0)\n",
      "step 209 loss: Value(data=5.053999988932407, grad=0.0)\n",
      "step 210 loss: Value(data=3.8581297854209695, grad=0.0)\n",
      "step 211 loss: Value(data=5.984415664700295, grad=0.0)\n",
      "step 212 loss: Value(data=5.593529705693055, grad=0.0)\n",
      "step 213 loss: Value(data=5.895607275404313, grad=0.0)\n",
      "step 214 loss: Value(data=2.045104296198498, grad=0.0)\n",
      "step 215 loss: Value(data=5.7599950918126686, grad=0.0)\n",
      "step 216 loss: Value(data=7.16508524377383, grad=0.0)\n",
      "step 217 loss: Value(data=4.943569828479356, grad=0.0)\n",
      "step 218 loss: Value(data=6.158038913775251, grad=0.0)\n",
      "step 219 loss: Value(data=5.808491446754801, grad=0.0)\n",
      "step 220 loss: Value(data=8.417496492310276, grad=0.0)\n",
      "step 221 loss: Value(data=6.1495321391041164, grad=0.0)\n",
      "step 222 loss: Value(data=4.791507978105509, grad=0.0)\n",
      "step 223 loss: Value(data=4.517617335807053, grad=0.0)\n",
      "step 224 loss: Value(data=4.439868797583472, grad=0.0)\n",
      "step 225 loss: Value(data=4.7607049423588315, grad=0.0)\n",
      "step 226 loss: Value(data=3.6390778915675153, grad=0.0)\n",
      "step 227 loss: Value(data=6.3526048184809945, grad=0.0)\n",
      "step 228 loss: Value(data=7.9359610722894995, grad=0.0)\n",
      "step 229 loss: Value(data=5.646602013117555, grad=0.0)\n",
      "step 230 loss: Value(data=13.126464419040802, grad=0.0)\n",
      "step 231 loss: Value(data=4.805322526112355, grad=0.0)\n",
      "step 232 loss: Value(data=7.8426750663369456, grad=0.0)\n",
      "step 233 loss: Value(data=2.6956473364697193, grad=0.0)\n",
      "step 234 loss: Value(data=6.467161961606016, grad=0.0)\n",
      "step 235 loss: Value(data=2.957289025547473, grad=0.0)\n",
      "step 236 loss: Value(data=5.581065050569352, grad=0.0)\n",
      "step 237 loss: Value(data=4.87303619457791, grad=0.0)\n",
      "step 238 loss: Value(data=6.738388609793056, grad=0.0)\n",
      "step 239 loss: Value(data=3.9659523557881204, grad=0.0)\n",
      "step 240 loss: Value(data=4.046564528703159, grad=0.0)\n",
      "step 241 loss: Value(data=6.304344668788598, grad=0.0)\n",
      "step 242 loss: Value(data=3.974500212711025, grad=0.0)\n",
      "step 243 loss: Value(data=3.409804778142653, grad=0.0)\n",
      "step 244 loss: Value(data=4.571055534709995, grad=0.0)\n",
      "step 245 loss: Value(data=4.566167170982236, grad=0.0)\n",
      "step 246 loss: Value(data=6.09581223108494, grad=0.0)\n",
      "step 247 loss: Value(data=5.6713856136249134, grad=0.0)\n",
      "step 248 loss: Value(data=8.309912980350434, grad=0.0)\n",
      "step 249 loss: Value(data=3.923363023037775, grad=0.0)\n",
      "step 250 loss: Value(data=3.828315102086317, grad=0.0)\n",
      "step 251 loss: Value(data=5.585269995634139, grad=0.0)\n",
      "step 252 loss: Value(data=3.4405745256362437, grad=0.0)\n",
      "step 253 loss: Value(data=8.885179524600437, grad=0.0)\n",
      "step 254 loss: Value(data=3.753786719320433, grad=0.0)\n",
      "step 255 loss: Value(data=2.182317077360218, grad=0.0)\n",
      "step 256 loss: Value(data=4.598678951833444, grad=0.0)\n",
      "step 257 loss: Value(data=5.256379473056961, grad=0.0)\n",
      "step 258 loss: Value(data=6.2881211001046, grad=0.0)\n",
      "step 259 loss: Value(data=3.006626893487896, grad=0.0)\n",
      "step 260 loss: Value(data=4.3764819809651945, grad=0.0)\n",
      "step 261 loss: Value(data=4.987212802704645, grad=0.0)\n",
      "step 262 loss: Value(data=1.7859816041852055, grad=0.0)\n",
      "step 263 loss: Value(data=5.4667659198346366, grad=0.0)\n",
      "step 264 loss: Value(data=2.352186858019893, grad=0.0)\n",
      "step 265 loss: Value(data=4.588695445829758, grad=0.0)\n",
      "step 266 loss: Value(data=3.649618290359835, grad=0.0)\n",
      "step 267 loss: Value(data=3.0917434161565107, grad=0.0)\n",
      "step 268 loss: Value(data=3.2265700461105227, grad=0.0)\n",
      "step 269 loss: Value(data=4.80753543969874, grad=0.0)\n",
      "step 270 loss: Value(data=9.17083446816269, grad=0.0)\n",
      "step 271 loss: Value(data=6.712908075312728, grad=0.0)\n",
      "step 272 loss: Value(data=3.534342616733178, grad=0.0)\n",
      "step 273 loss: Value(data=2.966596746953549, grad=0.0)\n",
      "step 274 loss: Value(data=3.320665639382923, grad=0.0)\n",
      "step 275 loss: Value(data=4.1281649772373985, grad=0.0)\n",
      "step 276 loss: Value(data=2.8895982284258612, grad=0.0)\n",
      "step 277 loss: Value(data=4.76958565162658, grad=0.0)\n",
      "step 278 loss: Value(data=3.8057009598451037, grad=0.0)\n",
      "step 279 loss: Value(data=3.7628745365934875, grad=0.0)\n",
      "step 280 loss: Value(data=1.5079312570628973, grad=0.0)\n",
      "step 281 loss: Value(data=4.988580531916368, grad=0.0)\n",
      "step 282 loss: Value(data=6.29146547762224, grad=0.0)\n",
      "step 283 loss: Value(data=2.7942033014715992, grad=0.0)\n",
      "step 284 loss: Value(data=2.7688489414374615, grad=0.0)\n",
      "step 285 loss: Value(data=5.348368393313572, grad=0.0)\n",
      "step 286 loss: Value(data=5.7268736987437645, grad=0.0)\n",
      "step 287 loss: Value(data=11.282350229585797, grad=0.0)\n",
      "step 288 loss: Value(data=3.5270064840948008, grad=0.0)\n",
      "step 289 loss: Value(data=5.936511410322998, grad=0.0)\n",
      "step 290 loss: Value(data=2.915776286744999, grad=0.0)\n",
      "step 291 loss: Value(data=5.284011541139994, grad=0.0)\n",
      "step 292 loss: Value(data=3.7218525713556803, grad=0.0)\n",
      "step 293 loss: Value(data=3.75428026372079, grad=0.0)\n",
      "step 294 loss: Value(data=1.790518921713387, grad=0.0)\n",
      "step 295 loss: Value(data=1.8672317203150721, grad=0.0)\n",
      "step 296 loss: Value(data=1.4691156853089573, grad=0.0)\n",
      "step 297 loss: Value(data=4.1815488594465124, grad=0.0)\n",
      "step 298 loss: Value(data=2.851611431859282, grad=0.0)\n",
      "step 299 loss: Value(data=6.321366927975408, grad=0.0)\n",
      "step 300 loss: Value(data=6.952238708766, grad=0.0)\n",
      "step 301 loss: Value(data=9.184354601900644, grad=0.0)\n",
      "step 302 loss: Value(data=2.1752300999016905, grad=0.0)\n",
      "step 303 loss: Value(data=3.7808286221221246, grad=0.0)\n",
      "step 304 loss: Value(data=2.913742391430105, grad=0.0)\n",
      "step 305 loss: Value(data=2.5551303977799744, grad=0.0)\n",
      "step 306 loss: Value(data=3.9244148972378308, grad=0.0)\n",
      "step 307 loss: Value(data=3.883017959208043, grad=0.0)\n",
      "step 308 loss: Value(data=3.4391355035260753, grad=0.0)\n",
      "step 309 loss: Value(data=5.456333202918194, grad=0.0)\n",
      "step 310 loss: Value(data=3.281643434891704, grad=0.0)\n",
      "step 311 loss: Value(data=4.177515974060407, grad=0.0)\n",
      "step 312 loss: Value(data=2.7414219248806204, grad=0.0)\n",
      "step 313 loss: Value(data=4.549735917596225, grad=0.0)\n",
      "step 314 loss: Value(data=3.293460991624475, grad=0.0)\n",
      "step 315 loss: Value(data=3.8631142334098727, grad=0.0)\n",
      "step 316 loss: Value(data=5.23778896630794, grad=0.0)\n",
      "step 317 loss: Value(data=3.3898002930270903, grad=0.0)\n",
      "step 318 loss: Value(data=1.3351530005146495, grad=0.0)\n",
      "step 319 loss: Value(data=1.7247621275507046, grad=0.0)\n",
      "step 320 loss: Value(data=3.2815121467161603, grad=0.0)\n",
      "step 321 loss: Value(data=7.402550820890538, grad=0.0)\n",
      "step 322 loss: Value(data=6.505091715152086, grad=0.0)\n",
      "step 323 loss: Value(data=3.5403947353988077, grad=0.0)\n",
      "step 324 loss: Value(data=4.022913605767887, grad=0.0)\n",
      "step 325 loss: Value(data=2.20258565259195, grad=0.0)\n",
      "step 326 loss: Value(data=2.7971605898571417, grad=0.0)\n",
      "step 327 loss: Value(data=2.4658551278072127, grad=0.0)\n",
      "step 328 loss: Value(data=2.927944266925696, grad=0.0)\n",
      "step 329 loss: Value(data=4.0194183220664295, grad=0.0)\n",
      "step 330 loss: Value(data=3.1221602486832865, grad=0.0)\n",
      "step 331 loss: Value(data=2.058752194017235, grad=0.0)\n",
      "step 332 loss: Value(data=1.933349916700044, grad=0.0)\n",
      "step 333 loss: Value(data=5.119269684520561, grad=0.0)\n",
      "step 334 loss: Value(data=4.694390345579843, grad=0.0)\n",
      "step 335 loss: Value(data=4.038545191720322, grad=0.0)\n",
      "step 336 loss: Value(data=2.589873101803138, grad=0.0)\n",
      "step 337 loss: Value(data=1.1266206965672896, grad=0.0)\n",
      "step 338 loss: Value(data=4.627841966705022, grad=0.0)\n",
      "step 339 loss: Value(data=1.6033550391539886, grad=0.0)\n",
      "step 340 loss: Value(data=4.169270170522317, grad=0.0)\n",
      "step 341 loss: Value(data=4.275406254834585, grad=0.0)\n",
      "step 342 loss: Value(data=2.5976215023084155, grad=0.0)\n",
      "step 343 loss: Value(data=1.9607787294160675, grad=0.0)\n",
      "step 344 loss: Value(data=2.2253559224035215, grad=0.0)\n",
      "step 345 loss: Value(data=1.9055018834840536, grad=0.0)\n",
      "step 346 loss: Value(data=3.349013401966223, grad=0.0)\n",
      "step 347 loss: Value(data=3.418152170396088, grad=0.0)\n",
      "step 348 loss: Value(data=2.2763609626662538, grad=0.0)\n",
      "step 349 loss: Value(data=2.9964338777716666, grad=0.0)\n",
      "step 350 loss: Value(data=4.45319097239843, grad=0.0)\n",
      "step 351 loss: Value(data=6.915317890437343, grad=0.0)\n",
      "step 352 loss: Value(data=3.2913390571200596, grad=0.0)\n",
      "step 353 loss: Value(data=1.9822740317627827, grad=0.0)\n",
      "step 354 loss: Value(data=3.1742426045668966, grad=0.0)\n",
      "step 355 loss: Value(data=3.3968121686651944, grad=0.0)\n",
      "step 356 loss: Value(data=2.640570127066261, grad=0.0)\n",
      "step 357 loss: Value(data=2.669119369419547, grad=0.0)\n",
      "step 358 loss: Value(data=2.6545981921515454, grad=0.0)\n",
      "step 359 loss: Value(data=4.815593428940863, grad=0.0)\n",
      "step 360 loss: Value(data=2.8393389979090546, grad=0.0)\n",
      "step 361 loss: Value(data=5.427026830894167, grad=0.0)\n",
      "step 362 loss: Value(data=3.3895132861682997, grad=0.0)\n",
      "step 363 loss: Value(data=2.2921229341780442, grad=0.0)\n",
      "step 364 loss: Value(data=2.1027944412187165, grad=0.0)\n",
      "step 365 loss: Value(data=5.094788683904395, grad=0.0)\n",
      "step 366 loss: Value(data=2.967346468433395, grad=0.0)\n",
      "step 367 loss: Value(data=3.0231877643269787, grad=0.0)\n",
      "step 368 loss: Value(data=1.801403774611826, grad=0.0)\n",
      "step 369 loss: Value(data=6.340779083381301, grad=0.0)\n",
      "step 370 loss: Value(data=4.068580904423127, grad=0.0)\n",
      "step 371 loss: Value(data=1.7342207471703184, grad=0.0)\n",
      "step 372 loss: Value(data=3.994428590097087, grad=0.0)\n",
      "step 373 loss: Value(data=2.371729690783218, grad=0.0)\n",
      "step 374 loss: Value(data=3.8571874386466365, grad=0.0)\n",
      "step 375 loss: Value(data=3.484823751778643, grad=0.0)\n",
      "step 376 loss: Value(data=2.506350908274308, grad=0.0)\n",
      "step 377 loss: Value(data=4.530605005242828, grad=0.0)\n",
      "step 378 loss: Value(data=2.194280368792305, grad=0.0)\n",
      "step 379 loss: Value(data=2.008675398400814, grad=0.0)\n",
      "step 380 loss: Value(data=6.30069755498226, grad=0.0)\n",
      "step 381 loss: Value(data=2.9709883878463503, grad=0.0)\n",
      "step 382 loss: Value(data=2.841037359205259, grad=0.0)\n",
      "step 383 loss: Value(data=2.354522926167488, grad=0.0)\n",
      "step 384 loss: Value(data=1.521393090110121, grad=0.0)\n",
      "step 385 loss: Value(data=3.4236272384295656, grad=0.0)\n",
      "step 386 loss: Value(data=3.7977832451962183, grad=0.0)\n",
      "step 387 loss: Value(data=2.861710726585024, grad=0.0)\n",
      "step 388 loss: Value(data=4.639515999319995, grad=0.0)\n",
      "step 389 loss: Value(data=1.8807121219378222, grad=0.0)\n",
      "step 390 loss: Value(data=2.2794887167917413, grad=0.0)\n",
      "step 391 loss: Value(data=3.3766687040019896, grad=0.0)\n",
      "step 392 loss: Value(data=5.129796311645382, grad=0.0)\n",
      "step 393 loss: Value(data=2.599674149570162, grad=0.0)\n",
      "step 394 loss: Value(data=3.7666672484155983, grad=0.0)\n",
      "step 395 loss: Value(data=3.463954418130902, grad=0.0)\n",
      "step 396 loss: Value(data=1.6382130477919021, grad=0.0)\n",
      "step 397 loss: Value(data=3.4476543879837456, grad=0.0)\n",
      "step 398 loss: Value(data=3.931864499636585, grad=0.0)\n",
      "step 399 loss: Value(data=3.9698560254941686, grad=0.0)\n",
      "step 400 loss: Value(data=1.6798421502180276, grad=0.0)\n",
      "step 401 loss: Value(data=2.5118839807837823, grad=0.0)\n",
      "step 402 loss: Value(data=3.2097840486402056, grad=0.0)\n",
      "step 403 loss: Value(data=1.8670672221564313, grad=0.0)\n",
      "step 404 loss: Value(data=2.8263144267385454, grad=0.0)\n",
      "step 405 loss: Value(data=3.1410362162015146, grad=0.0)\n",
      "step 406 loss: Value(data=2.339240410130633, grad=0.0)\n",
      "step 407 loss: Value(data=2.5280986194747785, grad=0.0)\n",
      "step 408 loss: Value(data=1.5908532556220583, grad=0.0)\n",
      "step 409 loss: Value(data=3.896451326858328, grad=0.0)\n",
      "step 410 loss: Value(data=2.89716333698219, grad=0.0)\n",
      "step 411 loss: Value(data=6.884931354585374, grad=0.0)\n",
      "step 412 loss: Value(data=3.8890911845369294, grad=0.0)\n",
      "step 413 loss: Value(data=1.4848143539413639, grad=0.0)\n",
      "step 414 loss: Value(data=3.245324428827262, grad=0.0)\n",
      "step 415 loss: Value(data=3.182480744794809, grad=0.0)\n",
      "step 416 loss: Value(data=1.8615197881590366, grad=0.0)\n",
      "step 417 loss: Value(data=1.9191951301817305, grad=0.0)\n",
      "step 418 loss: Value(data=3.294039626238474, grad=0.0)\n",
      "step 419 loss: Value(data=4.519891979391265, grad=0.0)\n",
      "step 420 loss: Value(data=3.098088888690372, grad=0.0)\n",
      "step 421 loss: Value(data=4.936290864743794, grad=0.0)\n",
      "step 422 loss: Value(data=2.5407539196230906, grad=0.0)\n",
      "step 423 loss: Value(data=2.000912565581154, grad=0.0)\n",
      "step 424 loss: Value(data=2.018316178587461, grad=0.0)\n",
      "step 425 loss: Value(data=4.579665538686489, grad=0.0)\n",
      "step 426 loss: Value(data=6.2223589743176175, grad=0.0)\n",
      "step 427 loss: Value(data=1.5159979206483434, grad=0.0)\n",
      "step 428 loss: Value(data=2.2140558886449537, grad=0.0)\n",
      "step 429 loss: Value(data=0.9567280109381768, grad=0.0)\n",
      "step 430 loss: Value(data=3.32848800457057, grad=0.0)\n",
      "step 431 loss: Value(data=1.9487400776172823, grad=0.0)\n",
      "step 432 loss: Value(data=1.7438841593516652, grad=0.0)\n",
      "step 433 loss: Value(data=2.5456360446666433, grad=0.0)\n",
      "step 434 loss: Value(data=2.3055774971089673, grad=0.0)\n",
      "step 435 loss: Value(data=2.5471031709623184, grad=0.0)\n",
      "step 436 loss: Value(data=2.641781625672032, grad=0.0)\n",
      "step 437 loss: Value(data=2.232732132879023, grad=0.0)\n",
      "step 438 loss: Value(data=2.025847791857965, grad=0.0)\n",
      "step 439 loss: Value(data=4.73388739938812, grad=0.0)\n",
      "step 440 loss: Value(data=2.554382425853175, grad=0.0)\n",
      "step 441 loss: Value(data=2.5668744896491225, grad=0.0)\n",
      "step 442 loss: Value(data=1.7924014577800385, grad=0.0)\n",
      "step 443 loss: Value(data=2.8677441018715797, grad=0.0)\n",
      "step 444 loss: Value(data=3.2261705047234526, grad=0.0)\n",
      "step 445 loss: Value(data=2.31875578221613, grad=0.0)\n",
      "step 446 loss: Value(data=2.672512068837963, grad=0.0)\n",
      "step 447 loss: Value(data=2.5322253375012003, grad=0.0)\n",
      "step 448 loss: Value(data=1.8097671965122206, grad=0.0)\n",
      "step 449 loss: Value(data=1.9158427181150472, grad=0.0)\n",
      "step 450 loss: Value(data=5.494159812948915, grad=0.0)\n",
      "step 451 loss: Value(data=1.5365309694271536, grad=0.0)\n",
      "step 452 loss: Value(data=2.7214173249071143, grad=0.0)\n",
      "step 453 loss: Value(data=2.2714068204480906, grad=0.0)\n",
      "step 454 loss: Value(data=1.7443080748608029, grad=0.0)\n",
      "step 455 loss: Value(data=0.8516628941882532, grad=0.0)\n",
      "step 456 loss: Value(data=1.6656064146929874, grad=0.0)\n",
      "step 457 loss: Value(data=2.1976169437745297, grad=0.0)\n",
      "step 458 loss: Value(data=2.484869804445014, grad=0.0)\n",
      "step 459 loss: Value(data=2.057936035185679, grad=0.0)\n",
      "step 460 loss: Value(data=4.420989375026935, grad=0.0)\n",
      "step 461 loss: Value(data=2.7997853031454007, grad=0.0)\n",
      "step 462 loss: Value(data=3.0357281699474568, grad=0.0)\n",
      "step 463 loss: Value(data=5.574676936828354, grad=0.0)\n",
      "step 464 loss: Value(data=2.6474168687326265, grad=0.0)\n",
      "step 465 loss: Value(data=3.327265099394688, grad=0.0)\n",
      "step 466 loss: Value(data=1.6978137496167611, grad=0.0)\n",
      "step 467 loss: Value(data=3.8005114119895396, grad=0.0)\n",
      "step 468 loss: Value(data=2.3488793292510337, grad=0.0)\n",
      "step 469 loss: Value(data=3.8952659474121147, grad=0.0)\n",
      "step 470 loss: Value(data=2.3376364239991547, grad=0.0)\n",
      "step 471 loss: Value(data=3.8867597318104705, grad=0.0)\n",
      "step 472 loss: Value(data=1.1642555293923802, grad=0.0)\n",
      "step 473 loss: Value(data=2.9835681213975165, grad=0.0)\n",
      "step 474 loss: Value(data=0.9781035282579215, grad=0.0)\n",
      "step 475 loss: Value(data=3.2102545836965755, grad=0.0)\n",
      "step 476 loss: Value(data=1.7094855890408807, grad=0.0)\n",
      "step 477 loss: Value(data=3.732835682821454, grad=0.0)\n",
      "step 478 loss: Value(data=3.4088022530075657, grad=0.0)\n",
      "step 479 loss: Value(data=1.6633884450968186, grad=0.0)\n",
      "step 480 loss: Value(data=2.301432787368332, grad=0.0)\n",
      "step 481 loss: Value(data=3.2324657839371644, grad=0.0)\n",
      "step 482 loss: Value(data=5.447320940774501, grad=0.0)\n",
      "step 483 loss: Value(data=2.9008935643253873, grad=0.0)\n",
      "step 484 loss: Value(data=2.047706984641083, grad=0.0)\n",
      "step 485 loss: Value(data=3.031445560508995, grad=0.0)\n",
      "step 486 loss: Value(data=2.1847505711559867, grad=0.0)\n",
      "step 487 loss: Value(data=4.026645314950017, grad=0.0)\n",
      "step 488 loss: Value(data=3.6476802283459073, grad=0.0)\n",
      "step 489 loss: Value(data=3.3515859081744526, grad=0.0)\n",
      "step 490 loss: Value(data=1.954088296862831, grad=0.0)\n",
      "step 491 loss: Value(data=2.8484943253364223, grad=0.0)\n",
      "step 492 loss: Value(data=2.4612039438793336, grad=0.0)\n",
      "step 493 loss: Value(data=2.0003689638268103, grad=0.0)\n",
      "step 494 loss: Value(data=2.5069225372866653, grad=0.0)\n",
      "step 495 loss: Value(data=1.348053078860663, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "eta = 0.0001\n",
    "for i in range(1, (size // batch_size) - batch_size): # 1 bc we already did one step. -batch_size for test set\n",
    "    x_batch = x[i*batch_size:i*batch_size + batch_size]\n",
    "    y_batch = y[i*batch_size:i*batch_size + batch_size]\n",
    "\n",
    "    ## forward pass\n",
    "    # running the model over the dataset\n",
    "    ypred = []\n",
    "    for j in range(batch_size):\n",
    "        ypred.append(model(x_batch[j]))\n",
    "    \n",
    "    # MSE loss function\n",
    "    loss_batch = [(yhat - ytrue)**2 for ytrue, yhat in zip(y_batch, ypred)]\n",
    "    loss = 0.0\n",
    "    for k in loss_batch: # sum() doesn't work for some reason so we've gotta do it manually\n",
    "        loss = loss + k\n",
    "    print(f'step {i} loss: {loss}')\n",
    "\n",
    "    ## backward pass\n",
    "    #set params to 0\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "    # clac gradients\n",
    "    loss.backward()\n",
    "    # performing a step of SGD\n",
    "    for p in model.parameters():\n",
    "        p.data += -eta * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7866c69-ac1b-432c-97c8-2ccfc92e16ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test loss: Value(data=1.5825152545418568, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "# final loss on test set\n",
    "x_test = x[-batch_size:]\n",
    "y_test = y[-batch_size:]\n",
    "\n",
    "# running the model over the dataset\n",
    "ypred = []\n",
    "for j in range(batch_size):\n",
    "    ypred.append(model(x_test[j]))\n",
    "\n",
    "# MSE loss function\n",
    "loss_batch = [(yhat - ytrue)**2 for ytrue, yhat in zip(y_test, ypred)]\n",
    "loss = 0.0\n",
    "for k in loss_batch: # sum() doesn't work for some reason so we've gotta do it manually\n",
    "    loss = loss + k\n",
    "print(f'final test loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf59efe1-e8c8-4671-b2cf-6f31885c4489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.9885137112017245, grad=0.056606532122643854),\n",
       " Value(data=0.9060762535548338, grad=-0.007508241012355071),\n",
       " Value(data=0.34734582642768086, grad=-0.002182098847789725),\n",
       " Value(data=0.17376199073753593, grad=0.019098846266687886),\n",
       " Value(data=-0.002900332621623437, grad=-0.07883886984272068),\n",
       " Value(data=-0.27918577942137945, grad=0.8476605020118341),\n",
       " Value(data=0.8273703587609359, grad=-1.1393260866048127),\n",
       " Value(data=0.2326768247480475, grad=1.088807727898387),\n",
       " Value(data=0.6373067818913496, grad=-0.05424270996255681),\n",
       " Value(data=-0.120899731533268, grad=-2.226895948158038),\n",
       " Value(data=0.8961667416211861, grad=-0.5691772495341849),\n",
       " Value(data=-0.8429762381978134, grad=-0.8561417915863436),\n",
       " Value(data=0.27366276877647955, grad=0.6513478670605626),\n",
       " Value(data=-0.8162255919269171, grad=-1.2154538194462228),\n",
       " Value(data=0.10192898544386289, grad=1.9181267133571152),\n",
       " Value(data=-0.7855258172395352, grad=-1.355718031831505),\n",
       " Value(data=0.433061340004016, grad=-0.4616573026444345),\n",
       " Value(data=0.2638437563211393, grad=0.2419958007083466),\n",
       " Value(data=-0.7946232399463438, grad=-1.1343046782971966),\n",
       " Value(data=0.018380544117726282, grad=2.7967563326346183),\n",
       " Value(data=-0.2150016574167572, grad=0.5646903142654248),\n",
       " Value(data=-0.10851154430860824, grad=-0.4906032758201),\n",
       " Value(data=-0.9678926263214326, grad=0.542892993898419),\n",
       " Value(data=-0.5700066281650416, grad=1.1624896320950246),\n",
       " Value(data=0.1796013107025911, grad=-2.486766133732452),\n",
       " Value(data=-0.2661175017636429, grad=-0.13227990873999654),\n",
       " Value(data=0.32528578534760677, grad=-0.10687237054704227),\n",
       " Value(data=0.8560511690232766, grad=0.11874968944788317),\n",
       " Value(data=-0.6180409866423324, grad=-0.14485696048042104),\n",
       " Value(data=-0.004797150064965926, grad=0.33684204055005373),\n",
       " Value(data=0.0007051614899150193, grad=0.2812764847707484),\n",
       " Value(data=0.858637155769393, grad=-0.4910138055887264),\n",
       " Value(data=-0.3498873894197242, grad=0.5594427570465184),\n",
       " Value(data=0.1258790710801302, grad=-0.003808902657838377),\n",
       " Value(data=0.01975966446428136, grad=-0.8044017718548551),\n",
       " Value(data=-0.6097298028026713, grad=1.5893848267142523),\n",
       " Value(data=0.09008275594862099, grad=-1.1103962768176363),\n",
       " Value(data=-1.2152682750786286, grad=1.328428430389564),\n",
       " Value(data=0.3245233133447772, grad=1.1322920087978554),\n",
       " Value(data=0.35943802898633526, grad=-3.7261010543847872),\n",
       " Value(data=-0.4974127992684697, grad=0.041556554231382034),\n",
       " Value(data=0.3264739578940243, grad=-0.3933654758042031),\n",
       " Value(data=-0.6191508817792954, grad=0.25746752705830506),\n",
       " Value(data=0.34793823994366296, grad=0.026915789228478842),\n",
       " Value(data=0.014636865434754981, grad=-0.23424201881966664),\n",
       " Value(data=-0.6967302571793325, grad=-0.48475611981159433),\n",
       " Value(data=-0.08347213864449611, grad=-0.033411485522276754),\n",
       " Value(data=0.3768860749473541, grad=0.07060903435497587),\n",
       " Value(data=-0.3148264722889624, grad=-0.4198885977346294),\n",
       " Value(data=0.020295272451456226, grad=0.832183523315094),\n",
       " Value(data=-0.3902623013234567, grad=2.0214399607415023),\n",
       " Value(data=0.18482555386545693, grad=0.3085215678717499),\n",
       " Value(data=0.35918218053079637, grad=0.02341532682108613),\n",
       " Value(data=-0.8411297872296065, grad=1.582523304003805),\n",
       " Value(data=-0.12449248010856728, grad=-4.076568743244839),\n",
       " Value(data=-0.5251260625100868, grad=-0.39480580907761),\n",
       " Value(data=0.3932582900994768, grad=0.6645532110609342),\n",
       " Value(data=-0.2241138351183334, grad=-0.5120371227173793),\n",
       " Value(data=0.3775667933946866, grad=0.02941083634098398),\n",
       " Value(data=-0.09053010578370849, grad=0.905607816964562),\n",
       " Value(data=-0.4288121284011948, grad=1.872912326558487),\n",
       " Value(data=-0.09104013761506535, grad=-0.5389328437204691),\n",
       " Value(data=-0.9423220127991507, grad=0.7951075438544437),\n",
       " Value(data=0.25840331602852473, grad=0.9351777160844833),\n",
       " Value(data=0.34291859907527117, grad=-3.816013049405061),\n",
       " Value(data=-0.48004956349664035, grad=-0.4889639703519675),\n",
       " Value(data=0.8928505671619714, grad=0.8220710470050486),\n",
       " Value(data=-0.3536663438223497, grad=-0.90785807117909),\n",
       " Value(data=0.8464055815973491, grad=0.005725344685249947),\n",
       " Value(data=-0.09030909968225145, grad=1.3684264481810975),\n",
       " Value(data=1.0741869826364772, grad=0.8640722472895699),\n",
       " Value(data=0.08666194268171809, grad=0.542015479904759),\n",
       " Value(data=0.17743441193217835, grad=-0.42015795074880913),\n",
       " Value(data=0.5507155886512342, grad=-0.5597190090891666),\n",
       " Value(data=0.12145628253474192, grad=2.466662380388219),\n",
       " Value(data=-1.0302297722133587, grad=1.511553337226713),\n",
       " Value(data=0.18780481235239962, grad=-0.1277436440801957),\n",
       " Value(data=0.6952014257611557, grad=-0.05907661267991565),\n",
       " Value(data=0.1787231102414275, grad=0.6283164200941219),\n",
       " Value(data=-0.1339648145746695, grad=-1.754954858135385),\n",
       " Value(data=0.9252819339086763, grad=-0.40876160439416953),\n",
       " Value(data=-0.42806415527132097, grad=-0.4755345008780918),\n",
       " Value(data=0.1720910572221155, grad=-0.04701103358219247),\n",
       " Value(data=0.6838642513545318, grad=0.15445400572894089),\n",
       " Value(data=-0.13617117105155144, grad=-0.12634789528034576),\n",
       " Value(data=-0.13972495991673511, grad=0.00374243699023997),\n",
       " Value(data=0.5746456707429191, grad=0.004036864471086537),\n",
       " Value(data=0.8613333978641744, grad=-0.005000988258135492),\n",
       " Value(data=-0.767512837520519, grad=0.006062357838324871),\n",
       " Value(data=-0.03961407492970661, grad=-0.014089124745787718),\n",
       " Value(data=-0.0930680037850424, grad=0.00886646057204762),\n",
       " Value(data=0.3363645570452191, grad=-0.021331878573870564),\n",
       " Value(data=-0.10943962047120712, grad=0.01465703533662391),\n",
       " Value(data=0.8670983334145201, grad=0.0013602753465526483),\n",
       " Value(data=-0.00014030599923013428, grad=-0.024309674496705402),\n",
       " Value(data=0.26381626621461196, grad=0.34399909477801643),\n",
       " Value(data=-0.44214006009665296, grad=0.2080881724536287),\n",
       " Value(data=0.2014779010714406, grad=-0.4177154177313023),\n",
       " Value(data=0.8243308409754266, grad=-0.03803204837433743),\n",
       " Value(data=-0.10829209611758894, grad=-0.5475460447227907),\n",
       " Value(data=0.12418404998683094, grad=-0.32157580718480744),\n",
       " Value(data=-0.17774195091018805, grad=-0.7101554426428037),\n",
       " Value(data=0.30329739708112463, grad=1.162263890121817),\n",
       " Value(data=-0.1442587616382694, grad=-1.1829559384600823),\n",
       " Value(data=0.22621537239510114, grad=2.1382275965543602),\n",
       " Value(data=-0.9260881734960436, grad=2.0991070516632266),\n",
       " Value(data=-0.04371138464410307, grad=0.09870715311248336),\n",
       " Value(data=0.34301844577968366, grad=-0.15414428063785113),\n",
       " Value(data=-0.04821516724550914, grad=1.027663470667617),\n",
       " Value(data=-0.0848823197016948, grad=-2.494995822496371),\n",
       " Value(data=-0.6511867766785032, grad=1.6574241701425096),\n",
       " Value(data=-0.34922443078608284, grad=0.08496042801625264),\n",
       " Value(data=0.2700106897612616, grad=-0.23082363568229247),\n",
       " Value(data=-0.6392675491789208, grad=1.6298002014754556),\n",
       " Value(data=-0.08951592970727824, grad=-3.470980024350923),\n",
       " Value(data=-0.8055517811950893, grad=-0.22929693917981175),\n",
       " Value(data=0.6793493456012945, grad=0.1287657315070994),\n",
       " Value(data=-0.2716945002467675, grad=-0.11163715902058387),\n",
       " Value(data=-0.8532041126737157, grad=-0.22443348538411026),\n",
       " Value(data=-0.021785018990163448, grad=0.5711558153726468),\n",
       " Value(data=-0.5994760936089435, grad=-1.1705594974876816),\n",
       " Value(data=-0.9305442435095352, grad=-0.7422541411668592),\n",
       " Value(data=0.3940530289885956, grad=1.0176544813389246),\n",
       " Value(data=0.5106028476689229, grad=-0.5769211173269825),\n",
       " Value(data=0.10703404860835361, grad=1.7187232435530913),\n",
       " Value(data=0.4804961089514112, grad=0.24181970824205928),\n",
       " Value(data=-0.1262010107558338, grad=0.3780749675192226),\n",
       " Value(data=-0.3899675837530838, grad=-0.3983306699505533),\n",
       " Value(data=0.4338799028418688, grad=0.21798937201359397),\n",
       " Value(data=-0.08927418506281663, grad=0.7533240045409807),\n",
       " Value(data=0.26662702887380857, grad=-0.8987220871228657),\n",
       " Value(data=-0.1867646836121259, grad=-1.0668673664236368),\n",
       " Value(data=1.0396273556690987, grad=1.22847012033123),\n",
       " Value(data=-0.3624623157806944, grad=-1.6013578702528077),\n",
       " Value(data=0.32516541568785773, grad=3.7362340822819116),\n",
       " Value(data=-0.18780771532261661, grad=0.2822150934859404),\n",
       " Value(data=-0.660351580964178, grad=0.4585228104378632),\n",
       " Value(data=-0.18393359054518627, grad=-0.5980757274486098),\n",
       " Value(data=-0.2162057689813551, grad=0.7119195186047602),\n",
       " Value(data=0.002407551548243622, grad=-1.1134412202992405),\n",
       " Value(data=-0.33820934251055274, grad=-0.8249675347692856),\n",
       " Value(data=0.18896234028460493, grad=0.05455964008220763),\n",
       " Value(data=0.1926795132264785, grad=0.08149710315106277),\n",
       " Value(data=0.1021416713222588, grad=-0.3332721886318097),\n",
       " Value(data=0.01368209448166991, grad=0.9534727463764285),\n",
       " Value(data=-0.7004777979906469, grad=0.43891350585782707),\n",
       " Value(data=0.39523293046033553, grad=-0.21287725129657042),\n",
       " Value(data=0.3426835962131498, grad=0.0211030825816992),\n",
       " Value(data=0.8221227348770828, grad=0.06729726187945782),\n",
       " Value(data=-0.040075399603743414, grad=-0.5024829452355042),\n",
       " Value(data=-0.4819616317780998, grad=-1.9206714835665477),\n",
       " Value(data=0.03608925345995355, grad=-0.1935020691331306),\n",
       " Value(data=0.3714423372170418, grad=0.4004311328261819),\n",
       " Value(data=-0.7040965025203059, grad=-2.0097274964264042),\n",
       " Value(data=0.09574143178688578, grad=3.917280372387785),\n",
       " Value(data=0.1089630660597148, grad=-0.15658620069612073),\n",
       " Value(data=0.20827111570912465, grad=0.15204295885601804),\n",
       " Value(data=-0.7593983517287858, grad=-0.2161861522396014),\n",
       " Value(data=-0.8670303102469973, grad=-0.421542028810972),\n",
       " Value(data=-0.057650869396199435, grad=0.8970143129519164),\n",
       " Value(data=-0.02707003027146828, grad=2.2337613313658053),\n",
       " Value(data=-0.7887619811335206, grad=0.8754953283766873),\n",
       " Value(data=0.6850843856392929, grad=2.302796369112075),\n",
       " Value(data=0.6246064891200436, grad=3.0128401724292595),\n",
       " Value(data=-0.6091836891091686, grad=2.7952668890478845),\n",
       " Value(data=0.10611720030343083, grad=1.7876797502280484),\n",
       " Value(data=-0.33462330426487646, grad=1.893924148598926),\n",
       " Value(data=-1.1993005056046124, grad=3.0469648363573496),\n",
       " Value(data=-0.17798528443637046, grad=1.7014371940032573),\n",
       " Value(data=0.2291744798712369, grad=2.250820235539599),\n",
       " Value(data=-0.8568650758329694, grad=1.8031529293003772),\n",
       " Value(data=0.456063383897652, grad=1.123922752427209),\n",
       " Value(data=-1.0762152633309916, grad=2.3869037083970177),\n",
       " Value(data=0.5561278624918757, grad=2.105585586681512),\n",
       " Value(data=1.073786271832665, grad=0.8312965413883399),\n",
       " Value(data=-0.6280556625320209, grad=2.029669512867563),\n",
       " Value(data=-0.6593877349719539, grad=0.09080760005480577),\n",
       " Value(data=-0.004538594501993155, grad=1.50532139155285),\n",
       " Value(data=-0.010799367141044519, grad=0.7855275114669478),\n",
       " Value(data=-0.5728800159190192, grad=0.10020481148877453),\n",
       " Value(data=0.7552957180582687, grad=1.4480903670852596),\n",
       " Value(data=-0.8933010679928411, grad=2.0577233250485785),\n",
       " Value(data=-0.8207395660463059, grad=2.316500545938297),\n",
       " Value(data=0.1587443242749011, grad=3.0280924788396404),\n",
       " Value(data=0.8706213904016028, grad=1.9331796667367123),\n",
       " Value(data=0.6431474559229329, grad=0.3906213186289231),\n",
       " Value(data=1.016868076071989, grad=2.982899026981953),\n",
       " Value(data=-0.4233063889177726, grad=0.9502215867950502),\n",
       " Value(data=0.36194709252498053, grad=0.7850268976338616),\n",
       " Value(data=-0.29354278111989274, grad=1.0524905495889185),\n",
       " Value(data=0.9991734723718954, grad=2.86140517799584),\n",
       " Value(data=0.23487175390971576, grad=2.095696824512472),\n",
       " Value(data=0.08963212940061259, grad=5.090635608365699)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667b482-8120-4d13-88f2-11ba9be10f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
