{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8a8e64-d964-4b59-91fb-b4d6edc8aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efef5b4-fe69-47c7-969b-6055bbf167fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# background info / review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65f80ae-5aa1-4da4-8eab-1739ee29bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709c0f30-619d-40e4-935f-066f27089e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c5ea12-13c5-4d7b-ab53-78fdb59d59dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1210e1490>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3572045e-74fb-4fe5-a2c0-4c9f9244cd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.00300000000243"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 0.001\n",
    "x = 3.0\n",
    "(f(x+h) - f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21d961-4b21-4bde-bce4-bb56b358da2e",
   "metadata": {},
   "source": [
    "# let's build micrograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b099d-2b60-4d90-b7da-bb9644ad3317",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## begin building class that keeps track of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a4e90c-ca34-491d-b43d-cc0bc886c5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __repr__(self): # repr defines how we should print() things\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Value(self.data + other.data)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Value(self.data - other.data)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Value(self.data * other.data)\n",
    "\n",
    "    def __truediv__(self, other): # as opposed to __floordiv__ the '//' symbol which is not differentiable\n",
    "        return Value(self.data / other.data)\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "c = Value(10.0)\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2377cf0f-9b70-4f6a-b346-8654fbc27e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value(data=-8.0), Value(data=-8.0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a-c, a.__sub__(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4019a97-390b-4ea1-88bd-7fa33e22f95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-30.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b*c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b993365a-db8a-4868-89c8-1d800a29598c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=5.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c/a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6348d70-4afb-4d43-9681-83ac5b36db9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## keeping track of the inputs to a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41f5a0b4-c5f3-4bf7-8a7a-2aa46ebdd158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=4.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=set(())): \n",
    "        self.data = data\n",
    "        self._prev = _children # set of an empty tuple, aka the empty set\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other): # passing in the values to children so they can be kept track of\n",
    "        return Value(self.data + other.data, (self, other))\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Value(self.data - other.data, (self, other))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Value(self.data * other.data, (self, other))\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return Value(self.data / other.data, (self, other))\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "c = Value(10.0)\n",
    "\n",
    "d = a*b + c\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fadc0e0a-10c9-4d29-a7c1-4585630020f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value(data=-6.0), Value(data=10.0))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d._prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f92755e-6953-4784-8d31-9c3b553937ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value(data=2.0), Value(data=-3.0))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d._prev[0]._prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c188d21-0f0a-4a03-aedb-1db8c7f5447b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## adding local gradient info\n",
    "\n",
    "this is not the actual global gradient that we're interested in; just a precursor concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1961f260-a476-462d-8675-eea1255f411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=set(()), local_grad=0.0): \n",
    "        self.data = data\n",
    "        self._prev = _children\n",
    "        self.local_grad = local_grad\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, local_grad={self.local_grad})\"\n",
    "\n",
    "    def __add__(self, other): \n",
    "        return Value(self.data + other.data, (self, other), 1)\n",
    "        # 1 rather than zero bc we're thinking about Value as a variable rather than as a constant\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Value(self.data - other.data, (self, other), 1)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Value(self.data * other.data, (self, other), other.data)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return Value(self.data / other.data, (self, other), 1/other.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2bdd334-ba8f-4ba9-ae37-b41d48d57efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=-6.0, local_grad=-3.0)\n",
      "Value(data=4.0, local_grad=1)\n",
      "Value(data=-0.6666666666666666, local_grad=-0.16666666666666666)\n"
     ]
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "c = Value(10.0)\n",
    "\n",
    "d = a*b # the first letter in the equation is the main character, second is the gradient\n",
    "e = d + c\n",
    "f = e / d\n",
    "\n",
    "print(d, e, f, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795234e-6e96-4d6d-8cd5-8d56102d2974",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## adding tanh nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02055737-3d97-4eb6-b666-f03d906a9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=set(()), local_grad=0.0): \n",
    "        self.data = data\n",
    "        self._prev = _children\n",
    "        self.local_grad = local_grad\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, local_grad={self.local_grad})\"\n",
    "\n",
    "    def __add__(self, other): \n",
    "        return Value(self.data + other.data, (self, other), 1)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Value(self.data - other.data, (self, other), 1)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Value(self.data * other.data, (self, other), other.data)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return Value(self.data / other.data, (self, other), 1/other.data)\n",
    "\n",
    "    def tanh(self):\n",
    "        t = (math.exp(2*self.data) - 1) / (math.exp(2*self.data) + 1)\n",
    "        return Value(t, (self, ), 1-t**2) # got that derivative of tanh from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c866aab-228c-4c5f-9835-0f61c907c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# miniature NN\n",
    "x1, x2 = Value(2.0), Value(0.0)\n",
    "w1, w2 = Value(-3.0), Value(1.0)\n",
    "b = Value(6.7)\n",
    "\n",
    "x1w1, x2w2 = x1*w1, x2*w2\n",
    "x1w1_plus_x2w2 = x1w1 + x2w2 # aka hidden neuron #1 before the bias & nonlinearity\n",
    "n = x1w1_plus_x2w2 + b\n",
    "o = n.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3260b4fc-7d5a-48aa-99ed-de91ec1b5e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=-6.0, local_grad=-3.0) Value(data=0.0, local_grad=1.0)\n",
      "Value(data=-6.0, local_grad=1)\n",
      "Value(data=0.7000000000000002, local_grad=1)\n",
      "Value(data=0.6043677771171636, local_grad=0.6347395899824584)\n"
     ]
    }
   ],
   "source": [
    "print(x1w1, x2w2)\n",
    "print(x1w1_plus_x2w2)\n",
    "print(n)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87b93f-a283-4bb2-b0eb-875d90404321",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## all prior labeled grad values were just the local gradients, not the actual backprop grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c478eb9d-fd2b-44d4-b089-574fb8e9f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=()): \n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self.grad = 0.0 # this no longer means local grad, it'll mean global grad from now on\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other): \n",
    "        out = Value(self.data + other.data, (self, other))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad # equivalent to multiplying by 1\n",
    "            other.grad += out.grad\n",
    "        # do this instead of _backward() because you just wanna store the funciton, not call it\n",
    "        out._backward = _backward \n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        out = Value(self.data - other.data, (self, other))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad -= out.grad\n",
    "            other.grad -= out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        out =  Value(self.data * other.data, (self, other))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        out = Value(self.data / other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1/other.data) * out.grad # i think that's right but i'm too lazy to check\n",
    "            other.grad += (1/self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        t = (math.exp(2*self.data) - 1) / (math.exp(2*self.data) + 1)\n",
    "        out =  Value(t, (self, )) \n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1-t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08b90b-0453-41c1-b92e-3e88c90ceaa0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### experiment/demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1115a8a7-0d70-4044-9938-6cc27a51d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# miniature NN\n",
    "x1, x2 = Value(2.0), Value(0.0)\n",
    "w1, w2 = Value(-3.0), Value(1.0)\n",
    "b = Value(6.8813735870195432) # this specific value makes the gradients less annoying looking\n",
    "\n",
    "x1w1, x2w2 = x1*w1, x2*w2\n",
    "x1w1_plus_x2w2 = x1w1 + x2w2\n",
    "n = x1w1_plus_x2w2 + b\n",
    "o = n.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b1d147a3-cb90-47fd-8b6c-1c3bd513d4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.7071067811865476, grad=0.0)\n",
      "Value(data=0.7071067811865476, grad=1.0)\n"
     ]
    }
   ],
   "source": [
    "# for the base case we set o.grad=1.0 so that we can see how it propogates\n",
    "print(o)\n",
    "o.grad=1.0\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "54c7619f-921b-40b5-8176-b846136e27e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.8813735870195432, grad=0.0)\n",
      "Value(data=0.8813735870195432, grad=0.4999999999999999)\n"
     ]
    }
   ],
   "source": [
    "print(n)\n",
    "o._backward()\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4e9f7d79-477f-4b7d-8697-44e7315890ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=-6.0, grad=0.0) \t\t Value(data=6.881373587019543, grad=0.0)\n",
      "Value(data=-6.0, grad=0.4999999999999999) Value(data=6.881373587019543, grad=0.4999999999999999)\n"
     ]
    }
   ],
   "source": [
    "print(x1w1_plus_x2w2, '\\t\\t', b)\n",
    "n._backward()\n",
    "print(x1w1_plus_x2w2, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e645c5b1-05d6-47b5-a711-ad74b79c242b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=-6.0, grad=0.0) \t\t Value(data=0.0, grad=0.0)\n",
      "Value(data=-6.0, grad=0.4999999999999999) Value(data=0.0, grad=0.4999999999999999)\n"
     ]
    }
   ],
   "source": [
    "print(x1w1, '\\t\\t', x2w2)\n",
    "x1w1_plus_x2w2._backward()\n",
    "print(x1w1, x2w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b7887fc9-de8f-4e5c-9aa5-4dd2609805d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=2.0, grad=0.0) \t\t Value(data=-3.0, grad=0.0)\n",
      "Value(data=2.0, grad=-1.4999999999999996) Value(data=-3.0, grad=0.9999999999999998)\n"
     ]
    }
   ],
   "source": [
    "print(x1, '\\t\\t', w1)\n",
    "x1w1._backward()\n",
    "print(x1, w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "edebd6e8-4b50-4233-950a-df75f25bb1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.0, grad=0.0) \t\t Value(data=1.0, grad=0.0)\n",
      "Value(data=0.0, grad=0.4999999999999999) Value(data=1.0, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "print(x2, '\\t\\t', w2)\n",
    "x2w2._backward()\n",
    "print(x2, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f62ad4-7e02-4c3b-8783-aa8cae69f118",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## making gradient automatically backprop thru computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "673c714f-0480-4501-8e72-2a835afce940",
   "metadata": {},
   "outputs": [],
   "source": [
    "topo = []\n",
    "visited = set()\n",
    "def build_topo(v): # topological sort maintains order\n",
    "    if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "            build_topo(child)\n",
    "        topo.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dfd2defa-d419-4bfc-83e7-895605e5d96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=1.0, grad=0.0),\n",
       " Value(data=0.0, grad=0.4999999999999999),\n",
       " Value(data=0.0, grad=0.4999999999999999),\n",
       " Value(data=2.0, grad=-1.4999999999999996),\n",
       " Value(data=-3.0, grad=0.9999999999999998),\n",
       " Value(data=-6.0, grad=0.4999999999999999),\n",
       " Value(data=-6.0, grad=0.4999999999999999),\n",
       " Value(data=6.881373587019543, grad=0.4999999999999999),\n",
       " Value(data=0.8813735870195432, grad=0.4999999999999999),\n",
       " Value(data=0.7071067811865476, grad=1.0)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_topo(o)\n",
    "topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "25d7bc80-5f52-4661-ab15-9bf7edbc1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=()): \n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self.grad = 0.0 \n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.3f}, grad={self.grad:.3f})\"\n",
    "\n",
    "    def __add__(self, other): \n",
    "        out = Value(self.data + other.data, (self, other))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad # equivalent to multiplying by 1\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward \n",
    "            # do this instead of _backward() because you just wanna store the funciton, not call it\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        out = Value(self.data - other.data, (self, other))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += -out.grad\n",
    "            other.grad += -out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        out =  Value(self.data * other.data, (self, other))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        out = Value(self.data / other.data, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1/other.data) * out.grad # i think that's right but i'm too lazy to check\n",
    "            other.grad += (1/self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        t = (math.exp(2*self.data) - 1) / (math.exp(2*self.data) + 1)\n",
    "        out =  Value(t, (self, )) \n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1-t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c27e3-853c-44de-8523-51e4e4b0e275",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### experiments/demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e0b12690-34d5-4877-875c-f1766e98a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# miniature NN\n",
    "x1, x2 = Value(2.0), Value(0.0)\n",
    "w1, w2 = Value(-3.0), Value(1.0)\n",
    "b = Value(6.8813735870195432) # this specific value makes the gradients less annoying looking\n",
    "\n",
    "x1w1, x2w2 = x1*w1, x2*w2\n",
    "x1w1_plus_x2w2 = x1w1 + x2w2\n",
    "n = x1w1_plus_x2w2 + b\n",
    "o = n.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c0021931-4750-4272-82da-37a42e9887d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.707, grad=1.000)\n",
      "Value(data=0.881, grad=0.500)\n",
      "Value(data=-6.000, grad=0.500) Value(data=6.881, grad=0.500)\n",
      "Value(data=-6.000, grad=0.500) Value(data=0.000, grad=0.500)\n",
      "Value(data=2.000, grad=-1.500) Value(data=-3.000, grad=1.000) Value(data=0.000, grad=0.500) Value(data=1.000, grad=0.000)\n"
     ]
    }
   ],
   "source": [
    "o.backward()\n",
    "print(o)\n",
    "print(n)\n",
    "print(x1w1_plus_x2w2, b)\n",
    "print(x1w1, x2w2)\n",
    "print(x1, w1, x2, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b346da-4df0-45e1-937c-ba9119fe2397",
   "metadata": {},
   "source": [
    "#### making sure gradients accumulate\n",
    "for any values that get put thru multiple operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f2c06c9a-4447-4a46-812d-59e03497eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=4.000, grad=1.000)\n",
      "Value(data=2.000, grad=2.000)\n"
     ]
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = a+a\n",
    "b.backward()\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77a800-37fd-4102-8de7-c908dccfd30c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## adding some more operations\n",
    "we'll also change a couple to take advantage of the new ones and be more general rather than being standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9156101b-6e3a-434a-9004-0f3234bb2c8e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"stores a single scalar value and its gradient\"\"\"\n",
    "    \n",
    "    def __init__(self, data, _children=()): \n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self.grad = 0.0 \n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.3f}, grad={self.grad:.3f})\"\n",
    "\n",
    "    def __add__(self, other): \n",
    "        other = other if isinstance(other, Value) else Value(other) \n",
    "        out = Value(self.data + other.data, (self, other))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad # equivalent to multiplying by 1\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward \n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other): # so that int + Value redirects to Value + int aka __add__. r stands for reverse\n",
    "        return self + other\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other): \n",
    "        return self + (-other) # so instead of writing its own we can just take advantage of __add__ and __neg__\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out =  Value(self.data * other.data, (self, other))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other): # so that int * Value redirects to Value * int aka __mul__. r stands for reverse\n",
    "        return self * other\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1 # simpler expression now that we have __pow__\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return self / other\n",
    "\n",
    "    def exp(self):\n",
    "        out = Value(math.exp(self.data), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad # local derivative of e^x is just e^x, aka out.data\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        out = ((2*self).exp() - 1) / ((2*self).exp() + 1)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1-out**2) * out.grad # i looked up the local gradient for tanh on wikipedia\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(max(0.0, self.data), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"backpropogates all gradients\"\"\"\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e2a0b6cc-754a-414b-847f-c190abd7c2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=4.000, grad=0.000)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "2*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d0f882f9-83b4-4124-b089-6ef64c153175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=7.389, grad=0.000)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "972edc31-7840-410a-8b7f-631087ebfa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.000, grad=0.000)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Value(4.0)\n",
    "b/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ee8cb50e-56d2-420f-b6d2-0d4f65d388e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.000, grad=0.000)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b67ac005-c965-44c2-bd32-f9ca32a95f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.999, grad=0.000)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "84e2c51e-f974-46db-825a-ac487cea028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.000, grad=0.000)\n"
     ]
    }
   ],
   "source": [
    "c = Value(-1.0)\n",
    "d = c.relu()\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ab100-d1a1-4b30-b065-ce999b1e65c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## making a very tiny NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "95af4acb-8026-480e-8acc-7970eab82cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-1.978, grad=0.000)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_dim):\n",
    "        self.w = [Value(r.uniform(-1,1)) for _ in range(input_dim)]\n",
    "        self.b = Value(r.uniform(-1,1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        assert len(x) == len(self.w), f'mismatch between input dim {len(x)} and weight dim {len(self.w)}'\n",
    "        # w * x + b\n",
    "        wixi = [wi*xi for wi, xi in zip(self.w, x)]\n",
    "        sum = Value(0.0)\n",
    "        for i in wixi:\n",
    "            sum = sum + i\n",
    "        act = sum + self.b\n",
    "        return act\n",
    "\n",
    "x = [2.0, 3.0]\n",
    "n = Neuron(len(x))\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8705189b-7aeb-4a8c-b9a2-005120d8e3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.464, grad=0.000),\n",
       " Value(data=2.094, grad=0.000),\n",
       " Value(data=-2.297, grad=0.000),\n",
       " Value(data=1.597, grad=0.000)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.neurons = [Neuron(input_dim) for _ in range(output_dim)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [n(x) for n in self.neurons]\n",
    "\n",
    "x = [2.0, 3.0]\n",
    "LL = LinearLayer(len(x), 4)\n",
    "LL(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b766f308-4d04-4b32-b26c-86072af21856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-1.026, grad=0.000), Value(data=1.631, grad=0.000)]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.up = LinearLayer(input_dim, hidden_dim)\n",
    "        self.down = LinearLayer(hidden_dim, output_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        up = self.up(x)\n",
    "        act = [i.relu() for i in up]\n",
    "        down = self.down(act)\n",
    "        return down\n",
    "\n",
    "x = [2.0, 3.0]\n",
    "model = MLP(len(x), 8, len(x))\n",
    "model(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b95ded58-d814-49ff-9ff3-fa3874d497d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a tiny dataset\n",
    "size = 16\n",
    "x = [[r.uniform(-1,1) for _ in range(4)] for _ in range(size)]\n",
    "y = []\n",
    "for features in x:\n",
    "    # Create a pattern: y = 2*x1 - 0.5*x2 + 3*x3 - x4 + noise\n",
    "    y_value = (\n",
    "        2 * features[0]\n",
    "        - 0.5 * features[1]\n",
    "        + 3 * features[2]\n",
    "        - features[3]\n",
    "        + r.uniform(-0.1, 0.1)  # Add some noise\n",
    "    )\n",
    "    y.append(y_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7e3bc538-be51-47f9-8383-675caeabfc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.500238115601044, -0.20765531671797421, 0.09196059650432198, -0.4293073252974877]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.228, grad=0.000)]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(len(x[0]), len(x[0])*4, 1) # initialize our model w/ hidden dimension of 4*input_dim\n",
    "print(x[0]) # taking a look at the input data\n",
    "model(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9a05308f-e5eb-4882-b4e2-39c267bd0385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=0.238, grad=0.000), Value(data=0.155, grad=0.000), Value(data=3.820, grad=0.000), Value(data=12.531, grad=0.000)]\n",
      "Value(data=16.744, grad=0.000)\n"
     ]
    }
   ],
   "source": [
    "# example of the MSE loss function\n",
    "ypred = []\n",
    "batch_size = 4\n",
    "for i in range(batch_size):\n",
    "    ypred.append(model(x[i])[0]) # the [0] removes the list around the final output\n",
    "loss_batch = [(yhat - ytrue)**2 for ytrue, yhat in zip(y, ypred)]\n",
    "print(loss_batch)\n",
    "loss = Value(0.0)\n",
    "for i in loss_batch: # sum() doesn't work for some reason so we've gotta do it manually\n",
    "    loss = loss + i\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "4e327d4c-8739-444c-bc18-1a3b62ed1cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.833, grad=0.000)\n",
      "Value(data=0.833, grad=-4.302)\n"
     ]
    }
   ],
   "source": [
    "print(model.up.neurons[0].w[0])\n",
    "loss.backward()\n",
    "print(model.up.neurons[0].w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b96c7-d0ea-4f31-9fb3-adad9db61bb7",
   "metadata": {},
   "source": [
    "## making it possible to take a step in the direction of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6bbd479b-8c58-4e4c-9a9f-43f195377524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_dim):\n",
    "        self.w = [Value(r.uniform(-1,1)) for _ in range(input_dim)]\n",
    "        self.b = Value(r.uniform(-1,1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        assert len(x) == len(self.w), f'mismatch between input dim {len(x)} and weight dim {len(self.w)}'\n",
    "        # w * x + b\n",
    "        wixi = [wi*xi for wi, xi in zip(self.w, x)]\n",
    "        sum = Value(0.0)\n",
    "        for i in wixi:\n",
    "            sum = sum + i\n",
    "        act = sum + self.b\n",
    "        return act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "class LinearLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.neurons = [Neuron(input_dim) for _ in range(output_dim)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [n(x) for n in self.neurons]\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.up = LinearLayer(input_dim, hidden_dim)\n",
    "        self.down = LinearLayer(hidden_dim, output_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        up = self.up(x)\n",
    "        act = [i.relu() for i in up]\n",
    "        down = self.down(act)\n",
    "        return down\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for p in self.up.parameters()] + [p for p in self.down.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ae723-eb99-47fa-b866-4be51d12efef",
   "metadata": {},
   "source": [
    "### experimental model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "546671b7-8edd-4ed1-a367-126135d08b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a tiny dataset\n",
    "size = 8192\n",
    "data_dim = 4\n",
    "x = [[r.uniform(-1,1) for _ in range(data_dim)] for _ in range(size)]\n",
    "y = []\n",
    "for features in x:\n",
    "    # Create a pattern: y = 2*x1 - 0.5*x2 + 3*x3 - x4 + noise\n",
    "    y_value = (\n",
    "        2 * features[0]\n",
    "        - 0.5 * features[1]\n",
    "        + 3 * features[2]\n",
    "        - features[3]\n",
    "        + r.uniform(-0.1, 0.1)  # Add some noise\n",
    "    )\n",
    "    y.append(y_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "2336581d-fe67-479c-aee4-47e0acad8a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.935, grad=0.000),\n",
       " Value(data=0.701, grad=0.000),\n",
       " Value(data=0.135, grad=0.000),\n",
       " Value(data=-0.491, grad=0.000),\n",
       " Value(data=-0.336, grad=0.000),\n",
       " Value(data=0.259, grad=0.000),\n",
       " Value(data=0.715, grad=0.000),\n",
       " Value(data=-0.467, grad=0.000),\n",
       " Value(data=-0.412, grad=0.000),\n",
       " Value(data=-0.153, grad=0.000),\n",
       " Value(data=-0.320, grad=0.000),\n",
       " Value(data=-0.809, grad=0.000),\n",
       " Value(data=-0.771, grad=0.000),\n",
       " Value(data=-0.610, grad=0.000),\n",
       " Value(data=-0.105, grad=0.000),\n",
       " Value(data=0.827, grad=0.000),\n",
       " Value(data=0.944, grad=0.000),\n",
       " Value(data=0.746, grad=0.000),\n",
       " Value(data=0.530, grad=0.000),\n",
       " Value(data=0.221, grad=0.000),\n",
       " Value(data=-0.894, grad=0.000),\n",
       " Value(data=-0.287, grad=0.000),\n",
       " Value(data=0.794, grad=0.000),\n",
       " Value(data=0.943, grad=0.000),\n",
       " Value(data=0.836, grad=0.000),\n",
       " Value(data=0.596, grad=0.000),\n",
       " Value(data=0.200, grad=0.000),\n",
       " Value(data=0.899, grad=0.000),\n",
       " Value(data=0.416, grad=0.000),\n",
       " Value(data=0.310, grad=0.000),\n",
       " Value(data=-0.822, grad=0.000),\n",
       " Value(data=0.381, grad=0.000),\n",
       " Value(data=-0.579, grad=0.000),\n",
       " Value(data=0.412, grad=0.000),\n",
       " Value(data=-0.509, grad=0.000),\n",
       " Value(data=0.907, grad=0.000),\n",
       " Value(data=-0.863, grad=0.000),\n",
       " Value(data=0.490, grad=0.000),\n",
       " Value(data=-0.824, grad=0.000),\n",
       " Value(data=-0.090, grad=0.000),\n",
       " Value(data=0.446, grad=0.000),\n",
       " Value(data=-0.408, grad=0.000),\n",
       " Value(data=0.147, grad=0.000),\n",
       " Value(data=-0.371, grad=0.000),\n",
       " Value(data=-0.238, grad=0.000),\n",
       " Value(data=0.956, grad=0.000),\n",
       " Value(data=0.226, grad=0.000),\n",
       " Value(data=0.488, grad=0.000),\n",
       " Value(data=0.781, grad=0.000),\n",
       " Value(data=0.070, grad=0.000),\n",
       " Value(data=0.188, grad=0.000),\n",
       " Value(data=0.248, grad=0.000),\n",
       " Value(data=-0.565, grad=0.000),\n",
       " Value(data=-0.738, grad=0.000),\n",
       " Value(data=0.261, grad=0.000),\n",
       " Value(data=0.759, grad=0.000),\n",
       " Value(data=-0.649, grad=0.000),\n",
       " Value(data=-0.357, grad=0.000),\n",
       " Value(data=0.193, grad=0.000),\n",
       " Value(data=-0.693, grad=0.000),\n",
       " Value(data=0.015, grad=0.000),\n",
       " Value(data=-0.185, grad=0.000),\n",
       " Value(data=0.102, grad=0.000),\n",
       " Value(data=0.206, grad=0.000),\n",
       " Value(data=0.249, grad=0.000),\n",
       " Value(data=-0.130, grad=0.000),\n",
       " Value(data=-0.347, grad=0.000),\n",
       " Value(data=-0.589, grad=0.000),\n",
       " Value(data=0.543, grad=0.000),\n",
       " Value(data=0.755, grad=0.000),\n",
       " Value(data=0.110, grad=0.000),\n",
       " Value(data=-0.968, grad=0.000),\n",
       " Value(data=-0.831, grad=0.000),\n",
       " Value(data=0.325, grad=0.000),\n",
       " Value(data=0.443, grad=0.000),\n",
       " Value(data=0.099, grad=0.000),\n",
       " Value(data=0.939, grad=0.000),\n",
       " Value(data=-0.916, grad=0.000),\n",
       " Value(data=-0.407, grad=0.000),\n",
       " Value(data=-0.449, grad=0.000),\n",
       " Value(data=-0.630, grad=0.000),\n",
       " Value(data=-0.775, grad=0.000),\n",
       " Value(data=-0.301, grad=0.000),\n",
       " Value(data=-0.206, grad=0.000),\n",
       " Value(data=-0.196, grad=0.000),\n",
       " Value(data=0.447, grad=0.000),\n",
       " Value(data=0.321, grad=0.000),\n",
       " Value(data=-0.071, grad=0.000),\n",
       " Value(data=-0.351, grad=0.000),\n",
       " Value(data=-0.688, grad=0.000),\n",
       " Value(data=0.924, grad=0.000),\n",
       " Value(data=0.762, grad=0.000),\n",
       " Value(data=-0.067, grad=0.000),\n",
       " Value(data=0.520, grad=0.000),\n",
       " Value(data=0.207, grad=0.000),\n",
       " Value(data=0.977, grad=0.000),\n",
       " Value(data=0.895, grad=0.000),\n",
       " Value(data=-0.107, grad=0.000),\n",
       " Value(data=0.067, grad=0.000),\n",
       " Value(data=0.855, grad=0.000),\n",
       " Value(data=0.986, grad=0.000),\n",
       " Value(data=0.038, grad=0.000),\n",
       " Value(data=0.711, grad=0.000),\n",
       " Value(data=-0.930, grad=0.000),\n",
       " Value(data=-0.260, grad=0.000),\n",
       " Value(data=0.511, grad=0.000),\n",
       " Value(data=0.428, grad=0.000),\n",
       " Value(data=-0.667, grad=0.000),\n",
       " Value(data=0.786, grad=0.000),\n",
       " Value(data=0.930, grad=0.000),\n",
       " Value(data=-0.400, grad=0.000),\n",
       " Value(data=-0.523, grad=0.000),\n",
       " Value(data=0.550, grad=0.000),\n",
       " Value(data=-0.745, grad=0.000),\n",
       " Value(data=-0.861, grad=0.000),\n",
       " Value(data=0.498, grad=0.000),\n",
       " Value(data=0.704, grad=0.000),\n",
       " Value(data=0.679, grad=0.000),\n",
       " Value(data=-0.135, grad=0.000),\n",
       " Value(data=0.749, grad=0.000),\n",
       " Value(data=-0.549, grad=0.000),\n",
       " Value(data=0.010, grad=0.000),\n",
       " Value(data=0.308, grad=0.000),\n",
       " Value(data=-0.782, grad=0.000),\n",
       " Value(data=0.293, grad=0.000),\n",
       " Value(data=-0.945, grad=0.000),\n",
       " Value(data=0.532, grad=0.000),\n",
       " Value(data=-0.174, grad=0.000),\n",
       " Value(data=0.585, grad=0.000),\n",
       " Value(data=-0.241, grad=0.000),\n",
       " Value(data=-0.417, grad=0.000),\n",
       " Value(data=-0.488, grad=0.000),\n",
       " Value(data=0.460, grad=0.000),\n",
       " Value(data=0.897, grad=0.000),\n",
       " Value(data=0.568, grad=0.000),\n",
       " Value(data=0.903, grad=0.000),\n",
       " Value(data=-0.882, grad=0.000),\n",
       " Value(data=-0.037, grad=0.000),\n",
       " Value(data=-0.359, grad=0.000),\n",
       " Value(data=0.657, grad=0.000),\n",
       " Value(data=-0.513, grad=0.000),\n",
       " Value(data=0.661, grad=0.000),\n",
       " Value(data=-0.787, grad=0.000),\n",
       " Value(data=-0.678, grad=0.000),\n",
       " Value(data=0.677, grad=0.000)]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(data_dim, data_dim*6, 1)\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "156515f3-5493-400b-a50a-54d47c4a63fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial test set loss: Value(data=342.320, grad=0.000)\n"
     ]
    }
   ],
   "source": [
    "# initial loss on test set\n",
    "batch_size = 16\n",
    "x_test = x[-batch_size:]\n",
    "y_test = y[-batch_size:]\n",
    "\n",
    "# running the model over the dataset\n",
    "ypred = []\n",
    "for j in range(batch_size):\n",
    "    ypred.append(model(x_test[j])[0]) # the [0] removes the list brackets surrounding the output\n",
    "\n",
    "# MSE loss function\n",
    "loss_batch = [(yhat - ytrue)**2 for ytrue, yhat in zip(y_test, ypred)]\n",
    "loss = Value(0.0)\n",
    "for k in loss_batch: # sum() doesn't work with Value for some reason so we've gotta do it manually\n",
    "    loss = loss + k\n",
    "    \n",
    "print(f'initial test set loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "16b58696-fc0b-4528-8c41-1c6b1490f10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 loss: Value(data=237.916, grad=0.000)\n",
      "step 2 loss: Value(data=265.199, grad=0.000)\n",
      "step 3 loss: Value(data=237.360, grad=0.000)\n",
      "step 4 loss: Value(data=203.557, grad=0.000)\n",
      "step 5 loss: Value(data=306.874, grad=0.000)\n",
      "step 6 loss: Value(data=205.269, grad=0.000)\n",
      "step 7 loss: Value(data=234.082, grad=0.000)\n",
      "step 8 loss: Value(data=233.806, grad=0.000)\n",
      "step 9 loss: Value(data=263.871, grad=0.000)\n",
      "step 10 loss: Value(data=435.120, grad=0.000)\n",
      "step 11 loss: Value(data=218.258, grad=0.000)\n",
      "step 12 loss: Value(data=198.980, grad=0.000)\n",
      "step 13 loss: Value(data=257.367, grad=0.000)\n",
      "step 14 loss: Value(data=169.828, grad=0.000)\n",
      "step 15 loss: Value(data=241.241, grad=0.000)\n",
      "step 16 loss: Value(data=209.698, grad=0.000)\n",
      "step 17 loss: Value(data=231.434, grad=0.000)\n",
      "step 18 loss: Value(data=336.162, grad=0.000)\n",
      "step 19 loss: Value(data=239.991, grad=0.000)\n",
      "step 20 loss: Value(data=159.772, grad=0.000)\n",
      "step 21 loss: Value(data=156.397, grad=0.000)\n",
      "step 22 loss: Value(data=216.487, grad=0.000)\n",
      "step 23 loss: Value(data=256.482, grad=0.000)\n",
      "step 24 loss: Value(data=182.779, grad=0.000)\n",
      "step 25 loss: Value(data=188.557, grad=0.000)\n",
      "step 26 loss: Value(data=280.476, grad=0.000)\n",
      "step 27 loss: Value(data=204.275, grad=0.000)\n",
      "step 28 loss: Value(data=205.734, grad=0.000)\n",
      "step 29 loss: Value(data=131.944, grad=0.000)\n",
      "step 30 loss: Value(data=250.668, grad=0.000)\n",
      "step 31 loss: Value(data=208.417, grad=0.000)\n",
      "step 32 loss: Value(data=139.125, grad=0.000)\n",
      "step 33 loss: Value(data=174.591, grad=0.000)\n",
      "step 34 loss: Value(data=267.499, grad=0.000)\n",
      "step 35 loss: Value(data=187.943, grad=0.000)\n",
      "step 36 loss: Value(data=208.329, grad=0.000)\n",
      "step 37 loss: Value(data=209.457, grad=0.000)\n",
      "step 38 loss: Value(data=157.511, grad=0.000)\n",
      "step 39 loss: Value(data=292.454, grad=0.000)\n",
      "step 40 loss: Value(data=195.939, grad=0.000)\n",
      "step 41 loss: Value(data=106.752, grad=0.000)\n",
      "step 42 loss: Value(data=186.237, grad=0.000)\n",
      "step 43 loss: Value(data=89.799, grad=0.000)\n",
      "step 44 loss: Value(data=170.248, grad=0.000)\n",
      "step 45 loss: Value(data=269.811, grad=0.000)\n",
      "step 46 loss: Value(data=170.886, grad=0.000)\n",
      "step 47 loss: Value(data=174.531, grad=0.000)\n",
      "step 48 loss: Value(data=144.288, grad=0.000)\n",
      "step 49 loss: Value(data=125.947, grad=0.000)\n",
      "step 50 loss: Value(data=111.646, grad=0.000)\n",
      "step 51 loss: Value(data=121.698, grad=0.000)\n",
      "step 52 loss: Value(data=148.067, grad=0.000)\n",
      "step 53 loss: Value(data=161.308, grad=0.000)\n",
      "step 54 loss: Value(data=155.021, grad=0.000)\n",
      "step 55 loss: Value(data=142.829, grad=0.000)\n",
      "step 56 loss: Value(data=132.994, grad=0.000)\n",
      "step 57 loss: Value(data=109.043, grad=0.000)\n",
      "step 58 loss: Value(data=87.414, grad=0.000)\n",
      "step 59 loss: Value(data=141.826, grad=0.000)\n",
      "step 60 loss: Value(data=95.280, grad=0.000)\n",
      "step 61 loss: Value(data=172.180, grad=0.000)\n",
      "step 62 loss: Value(data=98.794, grad=0.000)\n",
      "step 63 loss: Value(data=123.142, grad=0.000)\n",
      "step 64 loss: Value(data=104.476, grad=0.000)\n",
      "step 65 loss: Value(data=161.984, grad=0.000)\n",
      "step 66 loss: Value(data=183.069, grad=0.000)\n",
      "step 67 loss: Value(data=132.779, grad=0.000)\n",
      "step 68 loss: Value(data=117.857, grad=0.000)\n",
      "step 69 loss: Value(data=175.057, grad=0.000)\n",
      "step 70 loss: Value(data=119.434, grad=0.000)\n",
      "step 71 loss: Value(data=69.696, grad=0.000)\n",
      "step 72 loss: Value(data=80.752, grad=0.000)\n",
      "step 73 loss: Value(data=122.524, grad=0.000)\n",
      "step 74 loss: Value(data=142.063, grad=0.000)\n",
      "step 75 loss: Value(data=115.405, grad=0.000)\n",
      "step 76 loss: Value(data=153.025, grad=0.000)\n",
      "step 77 loss: Value(data=97.495, grad=0.000)\n",
      "step 78 loss: Value(data=137.565, grad=0.000)\n",
      "step 79 loss: Value(data=154.495, grad=0.000)\n",
      "step 80 loss: Value(data=135.712, grad=0.000)\n",
      "step 81 loss: Value(data=115.438, grad=0.000)\n",
      "step 82 loss: Value(data=79.870, grad=0.000)\n",
      "step 83 loss: Value(data=107.424, grad=0.000)\n",
      "step 84 loss: Value(data=84.423, grad=0.000)\n",
      "step 85 loss: Value(data=134.621, grad=0.000)\n",
      "step 86 loss: Value(data=95.805, grad=0.000)\n",
      "step 87 loss: Value(data=150.668, grad=0.000)\n",
      "step 88 loss: Value(data=116.629, grad=0.000)\n",
      "step 89 loss: Value(data=112.654, grad=0.000)\n",
      "step 90 loss: Value(data=71.802, grad=0.000)\n",
      "step 91 loss: Value(data=109.174, grad=0.000)\n",
      "step 92 loss: Value(data=85.450, grad=0.000)\n",
      "step 93 loss: Value(data=125.838, grad=0.000)\n",
      "step 94 loss: Value(data=65.267, grad=0.000)\n",
      "step 95 loss: Value(data=92.519, grad=0.000)\n",
      "step 96 loss: Value(data=106.979, grad=0.000)\n",
      "step 97 loss: Value(data=133.553, grad=0.000)\n",
      "step 98 loss: Value(data=101.696, grad=0.000)\n",
      "step 99 loss: Value(data=81.236, grad=0.000)\n",
      "step 100 loss: Value(data=92.941, grad=0.000)\n",
      "step 101 loss: Value(data=113.498, grad=0.000)\n",
      "step 102 loss: Value(data=92.935, grad=0.000)\n",
      "step 103 loss: Value(data=69.506, grad=0.000)\n",
      "step 104 loss: Value(data=63.557, grad=0.000)\n",
      "step 105 loss: Value(data=62.290, grad=0.000)\n",
      "step 106 loss: Value(data=65.825, grad=0.000)\n",
      "step 107 loss: Value(data=91.412, grad=0.000)\n",
      "step 108 loss: Value(data=69.682, grad=0.000)\n",
      "step 109 loss: Value(data=74.849, grad=0.000)\n",
      "step 110 loss: Value(data=80.824, grad=0.000)\n",
      "step 111 loss: Value(data=64.135, grad=0.000)\n",
      "step 112 loss: Value(data=141.864, grad=0.000)\n",
      "step 113 loss: Value(data=59.522, grad=0.000)\n",
      "step 114 loss: Value(data=58.480, grad=0.000)\n",
      "step 115 loss: Value(data=55.830, grad=0.000)\n",
      "step 116 loss: Value(data=81.374, grad=0.000)\n",
      "step 117 loss: Value(data=70.566, grad=0.000)\n",
      "step 118 loss: Value(data=106.576, grad=0.000)\n",
      "step 119 loss: Value(data=89.550, grad=0.000)\n",
      "step 120 loss: Value(data=80.775, grad=0.000)\n",
      "step 121 loss: Value(data=67.128, grad=0.000)\n",
      "step 122 loss: Value(data=73.276, grad=0.000)\n",
      "step 123 loss: Value(data=54.739, grad=0.000)\n",
      "step 124 loss: Value(data=49.926, grad=0.000)\n",
      "step 125 loss: Value(data=59.998, grad=0.000)\n",
      "step 126 loss: Value(data=87.138, grad=0.000)\n",
      "step 127 loss: Value(data=50.880, grad=0.000)\n",
      "step 128 loss: Value(data=86.173, grad=0.000)\n",
      "step 129 loss: Value(data=70.523, grad=0.000)\n",
      "step 130 loss: Value(data=70.877, grad=0.000)\n",
      "step 131 loss: Value(data=79.358, grad=0.000)\n",
      "step 132 loss: Value(data=101.492, grad=0.000)\n",
      "step 133 loss: Value(data=52.775, grad=0.000)\n",
      "step 134 loss: Value(data=34.065, grad=0.000)\n",
      "step 135 loss: Value(data=57.257, grad=0.000)\n",
      "step 136 loss: Value(data=68.060, grad=0.000)\n",
      "step 137 loss: Value(data=93.397, grad=0.000)\n",
      "step 138 loss: Value(data=19.677, grad=0.000)\n",
      "step 139 loss: Value(data=48.035, grad=0.000)\n",
      "step 140 loss: Value(data=38.652, grad=0.000)\n",
      "step 141 loss: Value(data=73.725, grad=0.000)\n",
      "step 142 loss: Value(data=75.522, grad=0.000)\n",
      "step 143 loss: Value(data=60.612, grad=0.000)\n",
      "step 144 loss: Value(data=101.770, grad=0.000)\n",
      "step 145 loss: Value(data=98.452, grad=0.000)\n",
      "step 146 loss: Value(data=48.239, grad=0.000)\n",
      "step 147 loss: Value(data=37.014, grad=0.000)\n",
      "step 148 loss: Value(data=33.690, grad=0.000)\n",
      "step 149 loss: Value(data=75.242, grad=0.000)\n",
      "step 150 loss: Value(data=86.041, grad=0.000)\n",
      "step 151 loss: Value(data=55.380, grad=0.000)\n",
      "step 152 loss: Value(data=44.830, grad=0.000)\n",
      "step 153 loss: Value(data=48.338, grad=0.000)\n",
      "step 154 loss: Value(data=48.868, grad=0.000)\n",
      "step 155 loss: Value(data=41.440, grad=0.000)\n",
      "step 156 loss: Value(data=38.236, grad=0.000)\n",
      "step 157 loss: Value(data=27.732, grad=0.000)\n",
      "step 158 loss: Value(data=78.930, grad=0.000)\n",
      "step 159 loss: Value(data=80.242, grad=0.000)\n",
      "step 160 loss: Value(data=47.348, grad=0.000)\n",
      "step 161 loss: Value(data=56.266, grad=0.000)\n",
      "step 162 loss: Value(data=50.897, grad=0.000)\n",
      "step 163 loss: Value(data=66.027, grad=0.000)\n",
      "step 164 loss: Value(data=28.838, grad=0.000)\n",
      "step 165 loss: Value(data=65.928, grad=0.000)\n",
      "step 166 loss: Value(data=52.245, grad=0.000)\n",
      "step 167 loss: Value(data=16.187, grad=0.000)\n",
      "step 168 loss: Value(data=65.167, grad=0.000)\n",
      "step 169 loss: Value(data=50.644, grad=0.000)\n",
      "step 170 loss: Value(data=27.733, grad=0.000)\n",
      "step 171 loss: Value(data=58.829, grad=0.000)\n",
      "step 172 loss: Value(data=40.664, grad=0.000)\n",
      "step 173 loss: Value(data=39.512, grad=0.000)\n",
      "step 174 loss: Value(data=28.939, grad=0.000)\n",
      "step 175 loss: Value(data=47.325, grad=0.000)\n",
      "step 176 loss: Value(data=42.584, grad=0.000)\n",
      "step 177 loss: Value(data=52.207, grad=0.000)\n",
      "step 178 loss: Value(data=31.821, grad=0.000)\n",
      "step 179 loss: Value(data=66.309, grad=0.000)\n",
      "step 180 loss: Value(data=38.229, grad=0.000)\n",
      "step 181 loss: Value(data=42.622, grad=0.000)\n",
      "step 182 loss: Value(data=35.375, grad=0.000)\n",
      "step 183 loss: Value(data=34.192, grad=0.000)\n",
      "step 184 loss: Value(data=33.349, grad=0.000)\n",
      "step 185 loss: Value(data=44.169, grad=0.000)\n",
      "step 186 loss: Value(data=33.725, grad=0.000)\n",
      "step 187 loss: Value(data=41.642, grad=0.000)\n",
      "step 188 loss: Value(data=32.563, grad=0.000)\n",
      "step 189 loss: Value(data=46.025, grad=0.000)\n",
      "step 190 loss: Value(data=35.980, grad=0.000)\n",
      "step 191 loss: Value(data=35.255, grad=0.000)\n",
      "step 192 loss: Value(data=51.399, grad=0.000)\n",
      "step 193 loss: Value(data=42.301, grad=0.000)\n",
      "step 194 loss: Value(data=46.192, grad=0.000)\n",
      "step 195 loss: Value(data=16.586, grad=0.000)\n",
      "step 196 loss: Value(data=49.132, grad=0.000)\n",
      "step 197 loss: Value(data=43.679, grad=0.000)\n",
      "step 198 loss: Value(data=34.646, grad=0.000)\n",
      "step 199 loss: Value(data=26.742, grad=0.000)\n",
      "step 200 loss: Value(data=51.808, grad=0.000)\n",
      "step 201 loss: Value(data=31.771, grad=0.000)\n",
      "step 202 loss: Value(data=23.454, grad=0.000)\n",
      "step 203 loss: Value(data=31.983, grad=0.000)\n",
      "step 204 loss: Value(data=18.891, grad=0.000)\n",
      "step 205 loss: Value(data=24.774, grad=0.000)\n",
      "step 206 loss: Value(data=40.919, grad=0.000)\n",
      "step 207 loss: Value(data=37.365, grad=0.000)\n",
      "step 208 loss: Value(data=41.509, grad=0.000)\n",
      "step 209 loss: Value(data=45.874, grad=0.000)\n",
      "step 210 loss: Value(data=47.107, grad=0.000)\n",
      "step 211 loss: Value(data=12.236, grad=0.000)\n",
      "step 212 loss: Value(data=31.395, grad=0.000)\n",
      "step 213 loss: Value(data=53.575, grad=0.000)\n",
      "step 214 loss: Value(data=31.737, grad=0.000)\n",
      "step 215 loss: Value(data=36.489, grad=0.000)\n",
      "step 216 loss: Value(data=16.481, grad=0.000)\n",
      "step 217 loss: Value(data=18.795, grad=0.000)\n",
      "step 218 loss: Value(data=22.373, grad=0.000)\n",
      "step 219 loss: Value(data=54.512, grad=0.000)\n",
      "step 220 loss: Value(data=44.780, grad=0.000)\n",
      "step 221 loss: Value(data=32.241, grad=0.000)\n",
      "step 222 loss: Value(data=28.756, grad=0.000)\n",
      "step 223 loss: Value(data=31.051, grad=0.000)\n",
      "step 224 loss: Value(data=29.463, grad=0.000)\n",
      "step 225 loss: Value(data=23.238, grad=0.000)\n",
      "step 226 loss: Value(data=24.282, grad=0.000)\n",
      "step 227 loss: Value(data=26.994, grad=0.000)\n",
      "step 228 loss: Value(data=26.409, grad=0.000)\n",
      "step 229 loss: Value(data=36.320, grad=0.000)\n",
      "step 230 loss: Value(data=24.503, grad=0.000)\n",
      "step 231 loss: Value(data=20.918, grad=0.000)\n",
      "step 232 loss: Value(data=25.726, grad=0.000)\n",
      "step 233 loss: Value(data=19.465, grad=0.000)\n",
      "step 234 loss: Value(data=19.071, grad=0.000)\n",
      "step 235 loss: Value(data=31.390, grad=0.000)\n",
      "step 236 loss: Value(data=27.250, grad=0.000)\n",
      "step 237 loss: Value(data=17.794, grad=0.000)\n",
      "step 238 loss: Value(data=24.270, grad=0.000)\n",
      "step 239 loss: Value(data=28.418, grad=0.000)\n",
      "step 240 loss: Value(data=65.575, grad=0.000)\n",
      "step 241 loss: Value(data=28.842, grad=0.000)\n",
      "step 242 loss: Value(data=16.665, grad=0.000)\n",
      "step 243 loss: Value(data=25.291, grad=0.000)\n",
      "step 244 loss: Value(data=34.098, grad=0.000)\n",
      "step 245 loss: Value(data=19.224, grad=0.000)\n",
      "step 246 loss: Value(data=21.639, grad=0.000)\n",
      "step 247 loss: Value(data=31.704, grad=0.000)\n",
      "step 248 loss: Value(data=12.032, grad=0.000)\n",
      "step 249 loss: Value(data=27.558, grad=0.000)\n",
      "step 250 loss: Value(data=29.019, grad=0.000)\n",
      "step 251 loss: Value(data=37.622, grad=0.000)\n",
      "step 252 loss: Value(data=20.208, grad=0.000)\n",
      "step 253 loss: Value(data=42.643, grad=0.000)\n",
      "step 254 loss: Value(data=19.618, grad=0.000)\n",
      "step 255 loss: Value(data=23.667, grad=0.000)\n",
      "step 256 loss: Value(data=29.439, grad=0.000)\n",
      "step 257 loss: Value(data=12.227, grad=0.000)\n",
      "step 258 loss: Value(data=25.435, grad=0.000)\n",
      "step 259 loss: Value(data=10.825, grad=0.000)\n",
      "step 260 loss: Value(data=17.879, grad=0.000)\n",
      "step 261 loss: Value(data=13.265, grad=0.000)\n",
      "step 262 loss: Value(data=23.837, grad=0.000)\n",
      "step 263 loss: Value(data=22.062, grad=0.000)\n",
      "step 264 loss: Value(data=16.804, grad=0.000)\n",
      "step 265 loss: Value(data=22.619, grad=0.000)\n",
      "step 266 loss: Value(data=21.440, grad=0.000)\n",
      "step 267 loss: Value(data=11.498, grad=0.000)\n",
      "step 268 loss: Value(data=25.558, grad=0.000)\n",
      "step 269 loss: Value(data=18.390, grad=0.000)\n",
      "step 270 loss: Value(data=24.853, grad=0.000)\n",
      "step 271 loss: Value(data=15.052, grad=0.000)\n",
      "step 272 loss: Value(data=14.324, grad=0.000)\n",
      "step 273 loss: Value(data=15.468, grad=0.000)\n",
      "step 274 loss: Value(data=21.299, grad=0.000)\n",
      "step 275 loss: Value(data=30.512, grad=0.000)\n",
      "step 276 loss: Value(data=9.872, grad=0.000)\n",
      "step 277 loss: Value(data=15.151, grad=0.000)\n",
      "step 278 loss: Value(data=16.537, grad=0.000)\n",
      "step 279 loss: Value(data=21.575, grad=0.000)\n",
      "step 280 loss: Value(data=8.061, grad=0.000)\n",
      "step 281 loss: Value(data=15.714, grad=0.000)\n",
      "step 282 loss: Value(data=20.078, grad=0.000)\n",
      "step 283 loss: Value(data=9.770, grad=0.000)\n",
      "step 284 loss: Value(data=18.129, grad=0.000)\n",
      "step 285 loss: Value(data=15.527, grad=0.000)\n",
      "step 286 loss: Value(data=10.835, grad=0.000)\n",
      "step 287 loss: Value(data=17.749, grad=0.000)\n",
      "step 288 loss: Value(data=12.052, grad=0.000)\n",
      "step 289 loss: Value(data=22.823, grad=0.000)\n",
      "step 290 loss: Value(data=9.342, grad=0.000)\n",
      "step 291 loss: Value(data=7.063, grad=0.000)\n",
      "step 292 loss: Value(data=12.708, grad=0.000)\n",
      "step 293 loss: Value(data=18.174, grad=0.000)\n",
      "step 294 loss: Value(data=14.375, grad=0.000)\n",
      "step 295 loss: Value(data=10.805, grad=0.000)\n",
      "step 296 loss: Value(data=24.914, grad=0.000)\n",
      "step 297 loss: Value(data=9.901, grad=0.000)\n",
      "step 298 loss: Value(data=5.027, grad=0.000)\n",
      "step 299 loss: Value(data=17.380, grad=0.000)\n",
      "step 300 loss: Value(data=9.836, grad=0.000)\n",
      "step 301 loss: Value(data=11.698, grad=0.000)\n",
      "step 302 loss: Value(data=22.919, grad=0.000)\n",
      "step 303 loss: Value(data=18.023, grad=0.000)\n",
      "step 304 loss: Value(data=11.196, grad=0.000)\n",
      "step 305 loss: Value(data=21.083, grad=0.000)\n",
      "step 306 loss: Value(data=24.031, grad=0.000)\n",
      "step 307 loss: Value(data=13.476, grad=0.000)\n",
      "step 308 loss: Value(data=8.279, grad=0.000)\n",
      "step 309 loss: Value(data=27.646, grad=0.000)\n",
      "step 310 loss: Value(data=6.235, grad=0.000)\n",
      "step 311 loss: Value(data=8.195, grad=0.000)\n",
      "step 312 loss: Value(data=11.374, grad=0.000)\n",
      "step 313 loss: Value(data=12.385, grad=0.000)\n",
      "step 314 loss: Value(data=20.908, grad=0.000)\n",
      "step 315 loss: Value(data=7.833, grad=0.000)\n",
      "step 316 loss: Value(data=14.018, grad=0.000)\n",
      "step 317 loss: Value(data=17.147, grad=0.000)\n",
      "step 318 loss: Value(data=7.808, grad=0.000)\n",
      "step 319 loss: Value(data=10.188, grad=0.000)\n",
      "step 320 loss: Value(data=9.936, grad=0.000)\n",
      "step 321 loss: Value(data=13.707, grad=0.000)\n",
      "step 322 loss: Value(data=17.722, grad=0.000)\n",
      "step 323 loss: Value(data=11.440, grad=0.000)\n",
      "step 324 loss: Value(data=18.753, grad=0.000)\n",
      "step 325 loss: Value(data=15.176, grad=0.000)\n",
      "step 326 loss: Value(data=10.141, grad=0.000)\n",
      "step 327 loss: Value(data=11.175, grad=0.000)\n",
      "step 328 loss: Value(data=10.423, grad=0.000)\n",
      "step 329 loss: Value(data=12.032, grad=0.000)\n",
      "step 330 loss: Value(data=14.149, grad=0.000)\n",
      "step 331 loss: Value(data=8.871, grad=0.000)\n",
      "step 332 loss: Value(data=5.751, grad=0.000)\n",
      "step 333 loss: Value(data=12.556, grad=0.000)\n",
      "step 334 loss: Value(data=15.376, grad=0.000)\n",
      "step 335 loss: Value(data=7.492, grad=0.000)\n",
      "step 336 loss: Value(data=12.754, grad=0.000)\n",
      "step 337 loss: Value(data=8.198, grad=0.000)\n",
      "step 338 loss: Value(data=7.855, grad=0.000)\n",
      "step 339 loss: Value(data=5.909, grad=0.000)\n",
      "step 340 loss: Value(data=14.484, grad=0.000)\n",
      "step 341 loss: Value(data=8.931, grad=0.000)\n",
      "step 342 loss: Value(data=8.333, grad=0.000)\n",
      "step 343 loss: Value(data=10.897, grad=0.000)\n",
      "step 344 loss: Value(data=9.117, grad=0.000)\n",
      "step 345 loss: Value(data=6.097, grad=0.000)\n",
      "step 346 loss: Value(data=6.615, grad=0.000)\n",
      "step 347 loss: Value(data=13.365, grad=0.000)\n",
      "step 348 loss: Value(data=7.036, grad=0.000)\n",
      "step 349 loss: Value(data=12.187, grad=0.000)\n",
      "step 350 loss: Value(data=7.253, grad=0.000)\n",
      "step 351 loss: Value(data=5.194, grad=0.000)\n",
      "step 352 loss: Value(data=7.363, grad=0.000)\n",
      "step 353 loss: Value(data=10.419, grad=0.000)\n",
      "step 354 loss: Value(data=4.343, grad=0.000)\n",
      "step 355 loss: Value(data=17.916, grad=0.000)\n",
      "step 356 loss: Value(data=5.550, grad=0.000)\n",
      "step 357 loss: Value(data=7.682, grad=0.000)\n",
      "step 358 loss: Value(data=8.009, grad=0.000)\n",
      "step 359 loss: Value(data=4.077, grad=0.000)\n",
      "step 360 loss: Value(data=7.208, grad=0.000)\n",
      "step 361 loss: Value(data=11.067, grad=0.000)\n",
      "step 362 loss: Value(data=11.064, grad=0.000)\n",
      "step 363 loss: Value(data=5.913, grad=0.000)\n",
      "step 364 loss: Value(data=11.814, grad=0.000)\n",
      "step 365 loss: Value(data=5.454, grad=0.000)\n",
      "step 366 loss: Value(data=7.687, grad=0.000)\n",
      "step 367 loss: Value(data=12.402, grad=0.000)\n",
      "step 368 loss: Value(data=15.426, grad=0.000)\n",
      "step 369 loss: Value(data=12.681, grad=0.000)\n",
      "step 370 loss: Value(data=6.541, grad=0.000)\n",
      "step 371 loss: Value(data=15.925, grad=0.000)\n",
      "step 372 loss: Value(data=7.060, grad=0.000)\n",
      "step 373 loss: Value(data=14.613, grad=0.000)\n",
      "step 374 loss: Value(data=5.368, grad=0.000)\n",
      "step 375 loss: Value(data=5.021, grad=0.000)\n",
      "step 376 loss: Value(data=6.234, grad=0.000)\n",
      "step 377 loss: Value(data=6.341, grad=0.000)\n",
      "step 378 loss: Value(data=8.875, grad=0.000)\n",
      "step 379 loss: Value(data=7.059, grad=0.000)\n",
      "step 380 loss: Value(data=7.330, grad=0.000)\n",
      "step 381 loss: Value(data=15.239, grad=0.000)\n",
      "step 382 loss: Value(data=8.320, grad=0.000)\n",
      "step 383 loss: Value(data=5.901, grad=0.000)\n",
      "step 384 loss: Value(data=4.818, grad=0.000)\n",
      "step 385 loss: Value(data=3.280, grad=0.000)\n",
      "step 386 loss: Value(data=2.463, grad=0.000)\n",
      "step 387 loss: Value(data=8.885, grad=0.000)\n",
      "step 388 loss: Value(data=7.970, grad=0.000)\n",
      "step 389 loss: Value(data=7.004, grad=0.000)\n",
      "step 390 loss: Value(data=10.477, grad=0.000)\n",
      "step 391 loss: Value(data=6.558, grad=0.000)\n",
      "step 392 loss: Value(data=7.806, grad=0.000)\n",
      "step 393 loss: Value(data=8.030, grad=0.000)\n",
      "step 394 loss: Value(data=6.469, grad=0.000)\n",
      "step 395 loss: Value(data=9.695, grad=0.000)\n",
      "step 396 loss: Value(data=9.063, grad=0.000)\n",
      "step 397 loss: Value(data=4.129, grad=0.000)\n",
      "step 398 loss: Value(data=4.011, grad=0.000)\n",
      "step 399 loss: Value(data=5.852, grad=0.000)\n",
      "step 400 loss: Value(data=7.765, grad=0.000)\n",
      "step 401 loss: Value(data=5.304, grad=0.000)\n",
      "step 402 loss: Value(data=1.915, grad=0.000)\n",
      "step 403 loss: Value(data=10.725, grad=0.000)\n",
      "step 404 loss: Value(data=3.266, grad=0.000)\n",
      "step 405 loss: Value(data=10.786, grad=0.000)\n",
      "step 406 loss: Value(data=11.748, grad=0.000)\n",
      "step 407 loss: Value(data=7.450, grad=0.000)\n",
      "step 408 loss: Value(data=12.311, grad=0.000)\n",
      "step 409 loss: Value(data=6.969, grad=0.000)\n",
      "step 410 loss: Value(data=6.440, grad=0.000)\n",
      "step 411 loss: Value(data=6.140, grad=0.000)\n",
      "step 412 loss: Value(data=4.239, grad=0.000)\n",
      "step 413 loss: Value(data=5.418, grad=0.000)\n",
      "step 414 loss: Value(data=5.580, grad=0.000)\n",
      "step 415 loss: Value(data=6.962, grad=0.000)\n",
      "step 416 loss: Value(data=5.752, grad=0.000)\n",
      "step 417 loss: Value(data=10.501, grad=0.000)\n",
      "step 418 loss: Value(data=4.712, grad=0.000)\n",
      "step 419 loss: Value(data=9.030, grad=0.000)\n",
      "step 420 loss: Value(data=8.683, grad=0.000)\n",
      "step 421 loss: Value(data=4.531, grad=0.000)\n",
      "step 422 loss: Value(data=4.550, grad=0.000)\n",
      "step 423 loss: Value(data=7.772, grad=0.000)\n",
      "step 424 loss: Value(data=5.584, grad=0.000)\n",
      "step 425 loss: Value(data=2.836, grad=0.000)\n",
      "step 426 loss: Value(data=3.942, grad=0.000)\n",
      "step 427 loss: Value(data=5.529, grad=0.000)\n",
      "step 428 loss: Value(data=2.816, grad=0.000)\n",
      "step 429 loss: Value(data=3.219, grad=0.000)\n",
      "step 430 loss: Value(data=3.227, grad=0.000)\n",
      "step 431 loss: Value(data=4.854, grad=0.000)\n",
      "step 432 loss: Value(data=5.387, grad=0.000)\n",
      "step 433 loss: Value(data=2.272, grad=0.000)\n",
      "step 434 loss: Value(data=4.372, grad=0.000)\n",
      "step 435 loss: Value(data=2.620, grad=0.000)\n",
      "step 436 loss: Value(data=4.100, grad=0.000)\n",
      "step 437 loss: Value(data=4.011, grad=0.000)\n",
      "step 438 loss: Value(data=6.384, grad=0.000)\n",
      "step 439 loss: Value(data=7.325, grad=0.000)\n",
      "step 440 loss: Value(data=3.490, grad=0.000)\n",
      "step 441 loss: Value(data=3.293, grad=0.000)\n",
      "step 442 loss: Value(data=4.891, grad=0.000)\n",
      "step 443 loss: Value(data=10.837, grad=0.000)\n",
      "step 444 loss: Value(data=4.982, grad=0.000)\n",
      "step 445 loss: Value(data=5.486, grad=0.000)\n",
      "step 446 loss: Value(data=4.415, grad=0.000)\n",
      "step 447 loss: Value(data=5.876, grad=0.000)\n",
      "step 448 loss: Value(data=3.515, grad=0.000)\n",
      "step 449 loss: Value(data=4.481, grad=0.000)\n",
      "step 450 loss: Value(data=2.516, grad=0.000)\n",
      "step 451 loss: Value(data=2.576, grad=0.000)\n",
      "step 452 loss: Value(data=5.733, grad=0.000)\n",
      "step 453 loss: Value(data=7.070, grad=0.000)\n",
      "step 454 loss: Value(data=5.151, grad=0.000)\n",
      "step 455 loss: Value(data=3.206, grad=0.000)\n",
      "step 456 loss: Value(data=4.897, grad=0.000)\n",
      "step 457 loss: Value(data=4.176, grad=0.000)\n",
      "step 458 loss: Value(data=6.116, grad=0.000)\n",
      "step 459 loss: Value(data=3.527, grad=0.000)\n",
      "step 460 loss: Value(data=6.646, grad=0.000)\n",
      "step 461 loss: Value(data=3.430, grad=0.000)\n",
      "step 462 loss: Value(data=2.734, grad=0.000)\n",
      "step 463 loss: Value(data=4.516, grad=0.000)\n",
      "step 464 loss: Value(data=5.416, grad=0.000)\n",
      "step 465 loss: Value(data=8.039, grad=0.000)\n",
      "step 466 loss: Value(data=5.495, grad=0.000)\n",
      "step 467 loss: Value(data=5.894, grad=0.000)\n",
      "step 468 loss: Value(data=4.041, grad=0.000)\n",
      "step 469 loss: Value(data=3.865, grad=0.000)\n",
      "step 470 loss: Value(data=4.512, grad=0.000)\n",
      "step 471 loss: Value(data=3.321, grad=0.000)\n",
      "step 472 loss: Value(data=3.351, grad=0.000)\n",
      "step 473 loss: Value(data=9.401, grad=0.000)\n",
      "step 474 loss: Value(data=6.437, grad=0.000)\n",
      "step 475 loss: Value(data=6.145, grad=0.000)\n",
      "step 476 loss: Value(data=2.951, grad=0.000)\n",
      "step 477 loss: Value(data=3.438, grad=0.000)\n",
      "step 478 loss: Value(data=3.147, grad=0.000)\n",
      "step 479 loss: Value(data=5.307, grad=0.000)\n",
      "step 480 loss: Value(data=5.210, grad=0.000)\n",
      "step 481 loss: Value(data=9.360, grad=0.000)\n",
      "step 482 loss: Value(data=3.566, grad=0.000)\n",
      "step 483 loss: Value(data=3.737, grad=0.000)\n",
      "step 484 loss: Value(data=3.793, grad=0.000)\n",
      "step 485 loss: Value(data=3.643, grad=0.000)\n",
      "step 486 loss: Value(data=6.991, grad=0.000)\n",
      "step 487 loss: Value(data=4.826, grad=0.000)\n",
      "step 488 loss: Value(data=1.665, grad=0.000)\n",
      "step 489 loss: Value(data=4.985, grad=0.000)\n",
      "step 490 loss: Value(data=4.359, grad=0.000)\n",
      "step 491 loss: Value(data=2.667, grad=0.000)\n",
      "step 492 loss: Value(data=3.109, grad=0.000)\n",
      "step 493 loss: Value(data=6.798, grad=0.000)\n",
      "step 494 loss: Value(data=5.683, grad=0.000)\n",
      "step 495 loss: Value(data=2.623, grad=0.000)\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "eta = 0.0001\n",
    "for i in range(1, (size // batch_size) - batch_size): # 1 bc we already did one step. -batch_size for test set\n",
    "    x_batch = x[i*batch_size:i*batch_size + batch_size]\n",
    "    y_batch = y[i*batch_size:i*batch_size + batch_size]\n",
    "\n",
    "    ## forward pass\n",
    "    # running the model over the dataset\n",
    "    ypred = []\n",
    "    for j in range(batch_size):\n",
    "        ypred.append(model(x_batch[j])[0]) # the [0] removes the list output\n",
    "    \n",
    "    # MSE loss function\n",
    "    loss_batch = [(yhat - ytrue)**2 for ytrue, yhat in zip(y_batch, ypred)]\n",
    "    loss = Value(0.0)\n",
    "    for k in loss_batch: # sum() doesn't work for some reason so we've gotta do it manually\n",
    "        loss = loss + k\n",
    "    print(f'step {i} loss: {loss}')\n",
    "\n",
    "    ## backward pass\n",
    "    #set params to 0\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "    # calc gradients\n",
    "    loss.backward()\n",
    "    # performing a step of SGD\n",
    "    for p in model.parameters():\n",
    "        p.data += -eta * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ddb011fd-36a4-4651-bfa5-d5153e4d3786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test loss: Value(data=2.545, grad=0.000)\n"
     ]
    }
   ],
   "source": [
    "# final loss on test set\n",
    "x_test = x[-batch_size:]\n",
    "y_test = y[-batch_size:]\n",
    "\n",
    "# running the model over the dataset\n",
    "ypred = []\n",
    "for j in range(batch_size):\n",
    "    ypred.append(model(x_test[j])[0]) # the [0] removes the list output\n",
    "\n",
    "# MSE loss function\n",
    "loss_batch = [(yhat - ytrue)**2 for ytrue, yhat in zip(y_test, ypred)]\n",
    "loss = Value(0.0)\n",
    "for k in loss_batch: # sum() doesn't work for some reason so we've gotta do it manually\n",
    "    loss = loss + k\n",
    "print(f'final test loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "15a7ce97-3039-4ddb-b9dd-822402f11e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.840, grad=-0.143),\n",
       " Value(data=0.662, grad=0.259),\n",
       " Value(data=-0.034, grad=0.159),\n",
       " Value(data=-0.450, grad=0.155),\n",
       " Value(data=-0.464, grad=-0.310),\n",
       " Value(data=0.241, grad=0.038),\n",
       " Value(data=0.722, grad=0.102),\n",
       " Value(data=-0.503, grad=0.064),\n",
       " Value(data=-0.400, grad=-0.088),\n",
       " Value(data=-0.136, grad=0.174),\n",
       " Value(data=-0.294, grad=-0.026),\n",
       " Value(data=-0.804, grad=-0.092),\n",
       " Value(data=-0.699, grad=-0.160),\n",
       " Value(data=-0.629, grad=0.359),\n",
       " Value(data=-0.177, grad=-0.006),\n",
       " Value(data=0.618, grad=-0.091),\n",
       " Value(data=0.993, grad=0.205),\n",
       " Value(data=0.381, grad=0.016),\n",
       " Value(data=0.691, grad=0.343),\n",
       " Value(data=0.174, grad=-0.724),\n",
       " Value(data=-0.825, grad=-0.064),\n",
       " Value(data=-0.291, grad=-0.143),\n",
       " Value(data=0.909, grad=-0.147),\n",
       " Value(data=0.886, grad=0.187),\n",
       " Value(data=0.804, grad=0.197),\n",
       " Value(data=0.363, grad=-0.286),\n",
       " Value(data=0.188, grad=0.380),\n",
       " Value(data=0.470, grad=-0.200),\n",
       " Value(data=0.666, grad=0.517),\n",
       " Value(data=0.139, grad=-0.848),\n",
       " Value(data=-0.683, grad=-0.353),\n",
       " Value(data=0.319, grad=-0.546),\n",
       " Value(data=-0.449, grad=-0.517),\n",
       " Value(data=0.326, grad=0.524),\n",
       " Value(data=-0.738, grad=0.626),\n",
       " Value(data=0.910, grad=0.159),\n",
       " Value(data=-0.874, grad=-0.063),\n",
       " Value(data=0.516, grad=0.044),\n",
       " Value(data=-0.841, grad=0.126),\n",
       " Value(data=-0.036, grad=-0.075),\n",
       " Value(data=0.562, grad=0.462),\n",
       " Value(data=-0.420, grad=-0.030),\n",
       " Value(data=0.451, grad=0.167),\n",
       " Value(data=-0.496, grad=0.268),\n",
       " Value(data=0.147, grad=-0.358),\n",
       " Value(data=0.934, grad=0.014),\n",
       " Value(data=0.222, grad=-0.103),\n",
       " Value(data=0.440, grad=0.027),\n",
       " Value(data=0.805, grad=-0.041),\n",
       " Value(data=0.057, grad=0.087),\n",
       " Value(data=-0.050, grad=-0.317),\n",
       " Value(data=0.326, grad=1.402),\n",
       " Value(data=-1.059, grad=1.553),\n",
       " Value(data=-0.433, grad=-2.692),\n",
       " Value(data=0.574, grad=-1.360),\n",
       " Value(data=0.771, grad=0.130),\n",
       " Value(data=-0.662, grad=0.806),\n",
       " Value(data=-0.423, grad=0.930),\n",
       " Value(data=0.227, grad=-0.511),\n",
       " Value(data=-0.644, grad=-1.455),\n",
       " Value(data=0.202, grad=0.431),\n",
       " Value(data=-0.156, grad=0.441),\n",
       " Value(data=0.495, grad=-0.095),\n",
       " Value(data=-0.014, grad=-0.045),\n",
       " Value(data=0.341, grad=0.252),\n",
       " Value(data=0.107, grad=0.099),\n",
       " Value(data=-0.414, grad=-0.238),\n",
       " Value(data=-0.137, grad=-0.387),\n",
       " Value(data=0.395, grad=0.526),\n",
       " Value(data=0.508, grad=0.620),\n",
       " Value(data=0.177, grad=-0.003),\n",
       " Value(data=-0.983, grad=0.004),\n",
       " Value(data=-0.678, grad=0.009),\n",
       " Value(data=0.266, grad=-0.011),\n",
       " Value(data=0.295, grad=-0.016),\n",
       " Value(data=0.178, grad=0.036),\n",
       " Value(data=0.784, grad=-0.065),\n",
       " Value(data=-0.673, grad=0.014),\n",
       " Value(data=-0.419, grad=0.630),\n",
       " Value(data=-0.741, grad=-0.529),\n",
       " Value(data=-0.883, grad=0.241),\n",
       " Value(data=-0.606, grad=0.220),\n",
       " Value(data=-0.907, grad=2.475),\n",
       " Value(data=0.144, grad=-3.995),\n",
       " Value(data=0.463, grad=-0.936),\n",
       " Value(data=0.446, grad=0.013),\n",
       " Value(data=0.320, grad=0.015),\n",
       " Value(data=-0.073, grad=0.002),\n",
       " Value(data=-0.350, grad=-0.017),\n",
       " Value(data=-0.690, grad=0.021),\n",
       " Value(data=0.774, grad=0.024),\n",
       " Value(data=0.784, grad=0.230),\n",
       " Value(data=-0.353, grad=0.417),\n",
       " Value(data=0.627, grad=0.137),\n",
       " Value(data=0.294, grad=-0.706),\n",
       " Value(data=1.286, grad=1.034),\n",
       " Value(data=0.774, grad=0.191),\n",
       " Value(data=0.587, grad=0.248),\n",
       " Value(data=-0.169, grad=0.798),\n",
       " Value(data=0.823, grad=-0.351),\n",
       " Value(data=0.913, grad=-0.029),\n",
       " Value(data=0.023, grad=0.011),\n",
       " Value(data=0.600, grad=-0.008),\n",
       " Value(data=-0.886, grad=-0.023),\n",
       " Value(data=-0.415, grad=0.014),\n",
       " Value(data=0.660, grad=0.019),\n",
       " Value(data=0.437, grad=0.120),\n",
       " Value(data=-0.421, grad=0.071),\n",
       " Value(data=0.707, grad=-0.086),\n",
       " Value(data=0.795, grad=-0.149),\n",
       " Value(data=-0.372, grad=-0.258),\n",
       " Value(data=-0.461, grad=-0.719),\n",
       " Value(data=0.473, grad=0.116),\n",
       " Value(data=-0.666, grad=-0.625),\n",
       " Value(data=-0.974, grad=0.893),\n",
       " Value(data=0.291, grad=-0.295),\n",
       " Value(data=0.788, grad=-0.023),\n",
       " Value(data=0.251, grad=0.025),\n",
       " Value(data=0.027, grad=-0.323),\n",
       " Value(data=0.805, grad=-0.018),\n",
       " Value(data=-0.355, grad=-0.338),\n",
       " Value(data=-0.150, grad=-0.412),\n",
       " Value(data=0.108, grad=-0.290),\n",
       " Value(data=-0.414, grad=-0.638),\n",
       " Value(data=0.108, grad=2.619),\n",
       " Value(data=-0.520, grad=-0.192),\n",
       " Value(data=0.344, grad=0.025),\n",
       " Value(data=0.322, grad=0.371),\n",
       " Value(data=0.851, grad=0.190),\n",
       " Value(data=0.087, grad=-0.294),\n",
       " Value(data=-0.944, grad=0.832),\n",
       " Value(data=-0.525, grad=0.008),\n",
       " Value(data=0.699, grad=0.083),\n",
       " Value(data=0.266, grad=2.575),\n",
       " Value(data=-0.006, grad=3.195),\n",
       " Value(data=0.736, grad=0.101),\n",
       " Value(data=-1.345, grad=2.674),\n",
       " Value(data=-0.033, grad=-0.061),\n",
       " Value(data=-0.441, grad=0.158),\n",
       " Value(data=1.110, grad=1.081),\n",
       " Value(data=-0.059, grad=0.279),\n",
       " Value(data=-0.052, grad=2.795),\n",
       " Value(data=-0.744, grad=-0.040),\n",
       " Value(data=-0.307, grad=0.396),\n",
       " Value(data=0.417, grad=1.693)]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc9469-1e30-40f3-a20a-9a0d5bccde3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
