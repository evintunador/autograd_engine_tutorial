{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r\n",
    "\n",
    "from engine import Value\n",
    "from modules import *\n",
    "from ops import *\n",
    "from gpt import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addition\n",
    "should we do modular addition instead??? problem with that is we'd have to use a huge modulo to have a reasonably sized training dataset right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_digits = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(k: int):\n",
    "    '''\n",
    "    creates a string representation of an arithmetic addition operation where the digits are in reverse-order\n",
    "    ex:\n",
    "    123 + 45 = 168 would be '321+54=861'\n",
    "    this reversal is meant to make the task easier on teh transformer\n",
    "    '''\n",
    "    max_num = int('9' * k)\n",
    "    num1 = r.randint(0, max_num)\n",
    "    num2 = r.randint(0, max_num)\n",
    "    num3 = num1 + num2\n",
    "    return f'b{str(num1)[::-1]}+{str(num2)[::-1]}={str(num3)[::-1]}e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = set()\n",
    "max_possible_dataset_size = int('9' * max_digits)**2\n",
    "dataset_size = 2\n",
    "i = 0\n",
    "while dataset_size < max_possible_dataset_size:\n",
    "    i += 1\n",
    "    dataset_size = 2 ** i\n",
    "dataset_size = 2 ** (i - 1)\n",
    "assert dataset_size < max_possible_dataset_size\n",
    "while len(dataset) < dataset_size:\n",
    "    dataset.add(generate_data(max_digits))\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7782\n",
      "410\n"
     ]
    }
   ],
   "source": [
    "split_size = int(0.95 * len(dataset))\n",
    "dataset = list(dataset)\n",
    "train_dataset = dataset[:split_size]\n",
    "val_dataset = dataset[split_size:]\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([12, 3, 4, 10, 7, 9, 11, 0, 4, 1], [3, 4, 10, 7, 9, 11, 0, 4, 1, 13])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = {\n",
    "    '0':0, '1':1, '2':2, '3':3, '4':4, '5':5, '6':6, '7':7, '8':8, '9':9, \n",
    "    '+':10, '=':11, \n",
    "    'b':12, # beginning of sequence token\n",
    "    'e':13, # end of sequence token\n",
    "    'p':14 # padding token\n",
    "}\n",
    "v = len(tokenizer)\n",
    "max_data_len = 4 + max_digits * 3 + 1\n",
    "def tokenize(equation):\n",
    "    out = []\n",
    "    for c in equation:\n",
    "        out.append(tokenizer[c])\n",
    "    while len(out) < max_data_len:\n",
    "        out.append(tokenizer['p'])\n",
    "    return out[:-1], out[1:]\n",
    "print(tokenize(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486\n",
      "25\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_iterations = split_size // batch_size\n",
    "print(train_iterations)\n",
    "val_iterations = (dataset_size - split_size) // batch_size\n",
    "print(val_iterations)\n",
    "val_frequency = train_iterations // val_iterations\n",
    "print(val_frequency)\n",
    "\n",
    "config = Config()\n",
    "config.max_seq_len = max_data_len - 1\n",
    "config.vocab_len = v\n",
    "model = GPT(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss: Value(data=433.288, grad=0.000)\n",
      "step 19 loss: Value(data=433.288, grad=0.000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(val_frequency + 1):#train_iterations):\n",
    "        \n",
    "    if i % val_frequency == 0:\n",
    "        val_batch_data = val_dataset[i % val_frequency:(i % val_frequency) + batch_size]\n",
    "        val_input_toks, val_target_toks = [], []\n",
    "        for entry in val_batch_data:\n",
    "            val_input_toks_seq, val_target_toks_seq = tokenize(entry)\n",
    "            val_input_toks.append(val_input_toks_seq)\n",
    "            val_target_toks.append(val_target_toks_seq)\n",
    "            \n",
    "        probabilities, loss = gpt(val_input_toks, val_target_toks)\n",
    "        print(f'step {i} loss: {loss}')\n",
    "        \n",
    "    train_batch_data = train_dataset[i:i + batch_size]\n",
    "    train_input_toks, train_target_toks = [], []\n",
    "    for entry in train_batch_data:\n",
    "        train_input_toks_seq, train_target_toks_seq = tokenize(entry)\n",
    "        train_input_toks.append(train_input_toks_seq)\n",
    "        train_target_toks.append(train_target_toks_seq)\n",
    "\n",
    "    probabilities, loss = model(train_input_toks, train_target_toks)\n",
    "\n",
    "    ## backward pass\n",
    "    #set params to 0\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "    # clac gradients\n",
    "    loss.backward()\n",
    "    # performing a step of SGD\n",
    "    for p in model.parameters():\n",
    "        p.data += -eta * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
