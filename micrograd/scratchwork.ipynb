{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r\n",
    "import time\n",
    "\n",
    "from engine import Value\n",
    "from modules import *\n",
    "from ops import *\n",
    "from gpt import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addition\n",
    "should we do modular addition instead??? problem with that is we'd have to use a huge modulo to have a reasonably sized training dataset right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_digits = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(k: int):\n",
    "    '''\n",
    "    creates a string representation of an arithmetic addition operation where the digits are in reverse-order\n",
    "    ex:\n",
    "    123 + 45 = 168 would be '321+54=861'\n",
    "    this reversal is meant to make the task easier on teh transformer\n",
    "    '''\n",
    "    max_num = int('9' * k)\n",
    "    num1 = r.randint(0, max_num)\n",
    "    num2 = r.randint(0, max_num)\n",
    "    num3 = num1 + num2\n",
    "    return f'b{str(num1)[::-1]}+{str(num2)[::-1]}={str(num3)[::-1]}e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = set()\n",
    "max_possible_dataset_size = int('9' * max_digits)**2\n",
    "dataset_size = 2\n",
    "i = 0\n",
    "while dataset_size < max_possible_dataset_size:\n",
    "    i += 1\n",
    "    dataset_size = 2 ** i\n",
    "dataset_size = 2 ** (i - 1)\n",
    "assert dataset_size < max_possible_dataset_size\n",
    "while len(dataset) < dataset_size:\n",
    "    dataset.add(generate_data(max_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7782\n",
      "410\n"
     ]
    }
   ],
   "source": [
    "split_size = int(0.95 * len(dataset))\n",
    "dataset = list(dataset)\n",
    "train_dataset = dataset[:split_size]\n",
    "val_dataset = dataset[split_size:]\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b38+84=131e\n",
      "[12, 3, 8, 10, 8, 4, 11, 1, 3, 1, 13]\n",
      "[12, 3, 8, 10, 8, 4, 11, 1, 3, 1] [3, 8, 10, 8, 4, 11, 1, 3, 1, 13]\n",
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: '+', 11: '=', 12: 'b', 13: 'e', 14: 'p'}\n",
      "83+48=131\n"
     ]
    }
   ],
   "source": [
    "tokenizer = {\n",
    "    '0':0, '1':1, '2':2, '3':3, '4':4, '5':5, '6':6, '7':7, '8':8, '9':9, \n",
    "    '+':10, '=':11, \n",
    "    'b':12, # beginning of sequence token\n",
    "    'e':13, # end of sequence token\n",
    "    'p':14 # padding token\n",
    "}\n",
    "v = len(tokenizer)\n",
    "max_data_len = 4 + max_digits * 3 + 1\n",
    "\n",
    "def tokenize(equation):\n",
    "    out = []\n",
    "    for c in equation:\n",
    "        out.append(tokenizer[c])\n",
    "    while len(out) < max_data_len:\n",
    "        out.append(tokenizer['p'])\n",
    "    return out\n",
    "\n",
    "print(train_dataset[0])\n",
    "tokens = tokenize(train_dataset[0])\n",
    "print(tokens)\n",
    "\n",
    "input_tokens, target_tokens = tokens[:-1], tokens[1:]\n",
    "print(input_tokens, target_tokens)\n",
    "\n",
    "reverse_tokenizer = {val:tok for tok, val in zip(tokenizer, tokenizer.values())}\n",
    "print(reverse_tokenizer)\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    char_list = [reverse_tokenizer[t] for t in tokens]\n",
    "    eos_idx = char_list.index('e')\n",
    "    chars = \"\".join(char_list[1:eos_idx])\n",
    "    num1, temp = chars.split('+')\n",
    "    num2, num3 = temp.split('=')\n",
    "    return f'{num1[::-1]}+{num2[::-1]}={num3[::-1]}'\n",
    "    \n",
    "output = decode_tokens(tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486 25 19\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_iterations = split_size // batch_size\n",
    "val_iterations = (dataset_size - split_size) // batch_size\n",
    "val_frequency = train_iterations // val_iterations\n",
    "print(train_iterations, val_iterations, val_frequency)\n",
    "\n",
    "config = {\n",
    "    'vocab_len':v,\n",
    "    'model_dim':4,\n",
    "    'max_seq_len':max_data_len - 1,\n",
    "    'num_heads':2,\n",
    "    'head_dim':2,\n",
    "    'mlp_mult':2,\n",
    "    'dropout_rate':0.1,\n",
    "    'num_layers':1\n",
    "}\n",
    "model = GPT(config)\n",
    "\n",
    "eta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 train loss: Value(data=433.288, grad=0.000) val loss: Value(data=433.288, grad=0.000) time: 2.5095489025115967\n",
      "step 19 train loss: Value(data=419.274, grad=0.000) val loss: Value(data=423.468, grad=0.000) time: 44.46553683280945\n",
      "step 38 train loss: Value(data=418.610, grad=0.000) val loss: Value(data=421.179, grad=0.000) time: 89.63981699943542\n",
      "step 57 train loss: Value(data=415.949, grad=0.000) val loss: Value(data=423.663, grad=0.000) time: 133.78260684013367\n",
      "step 76 train loss: Value(data=416.133, grad=0.000) val loss: Value(data=420.213, grad=0.000) time: 180.54873394966125\n",
      "step 95 train loss: Value(data=412.627, grad=0.000) val loss: Value(data=420.394, grad=0.000) time: 226.94694471359253\n",
      "step 114 train loss: Value(data=419.604, grad=0.000) val loss: Value(data=420.669, grad=0.000) time: 272.7566969394684\n",
      "step 133 train loss: Value(data=413.515, grad=0.000) val loss: Value(data=418.737, grad=0.000) time: 318.67963886260986\n",
      "step 152 train loss: Value(data=415.829, grad=0.000) val loss: Value(data=420.789, grad=0.000) time: 365.60509395599365\n",
      "step 171 train loss: Value(data=416.421, grad=0.000) val loss: Value(data=420.009, grad=0.000) time: 412.37292885780334\n",
      "step 190 train loss: Value(data=415.175, grad=0.000) val loss: Value(data=418.582, grad=0.000) time: 458.1136929988861\n",
      "step 209 train loss: Value(data=414.490, grad=0.000) val loss: Value(data=421.009, grad=0.000) time: 505.21893310546875\n",
      "step 228 train loss: Value(data=408.414, grad=0.000) val loss: Value(data=421.259, grad=0.000) time: 552.4340498447418\n",
      "step 247 train loss: Value(data=414.146, grad=0.000) val loss: Value(data=421.321, grad=0.000) time: 601.0659699440002\n",
      "step 266 train loss: Value(data=411.842, grad=0.000) val loss: Value(data=422.207, grad=0.000) time: 648.3603479862213\n",
      "step 285 train loss: Value(data=413.551, grad=0.000) val loss: Value(data=422.468, grad=0.000) time: 696.2017648220062\n",
      "step 304 train loss: Value(data=404.556, grad=0.000) val loss: Value(data=420.052, grad=0.000) time: 743.1580829620361\n",
      "step 323 train loss: Value(data=410.519, grad=0.000) val loss: Value(data=421.188, grad=0.000) time: 790.4106769561768\n",
      "step 342 train loss: Value(data=413.846, grad=0.000) val loss: Value(data=422.007, grad=0.000) time: 838.0520730018616\n",
      "step 361 train loss: Value(data=416.367, grad=0.000) val loss: Value(data=421.056, grad=0.000) time: 885.9106678962708\n",
      "step 380 train loss: Value(data=408.646, grad=0.000) val loss: Value(data=412.470, grad=0.000) time: 932.2041549682617\n",
      "step 399 train loss: Value(data=366.501, grad=0.000) val loss: Value(data=385.131, grad=0.000) time: 978.5041928291321\n",
      "step 418 train loss: Value(data=398.355, grad=0.000) val loss: Value(data=402.675, grad=0.000) time: 1025.1800479888916\n",
      "step 437 train loss: Value(data=355.507, grad=0.000) val loss: Value(data=357.863, grad=0.000) time: 1071.1378109455109\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     train_input_toks\u001b[38;5;241m.\u001b[39mappend(train_toks_seq[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      9\u001b[0m     train_target_toks\u001b[38;5;241m.\u001b[39mappend(train_toks_seq[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 11\u001b[0m probabilities, train_loss \u001b[38;5;241m=\u001b[39m model(train_input_toks, train_target_toks)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m val_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m     val_batch_data \u001b[38;5;241m=\u001b[39m val_dataset[i \u001b[38;5;241m%\u001b[39m val_frequency:(i \u001b[38;5;241m%\u001b[39m val_frequency) \u001b[38;5;241m+\u001b[39m batch_size]\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/gpt.py:210\u001b[0m, in \u001b[0;36mGPT.__call__\u001b[0;34m(self, input_token_ids, target_token_ids)\u001b[0m\n\u001b[1;32m    207\u001b[0m logits \u001b[38;5;241m=\u001b[39m vector_wise_apply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj, vector_wise_apply(layer_norm, x))\n\u001b[1;32m    208\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m vector_wise_apply(softmax, logits)\n\u001b[0;32m--> 210\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(probabilities, target_token_ids) \u001b[38;5;28;01mif\u001b[39;00m target_token_ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probabilities, loss\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/modules.py:89\u001b[0m, in \u001b[0;36mCrossEntropyLoss.__call__\u001b[0;34m(self, probabilities, targets)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# prolly should assert that each vec in probabilities is a valid distribution (sums to 1), but i'm lazy\u001b[39;00m\n\u001b[1;32m     88\u001b[0m one_hots \u001b[38;5;241m=\u001b[39m vector_wise_apply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_one_hot, targets)\n\u001b[0;32m---> 89\u001b[0m log_probabilities \u001b[38;5;241m=\u001b[39m vector_wise_apply(log, probabilities)\n\u001b[1;32m     90\u001b[0m individual_losses \u001b[38;5;241m=\u001b[39m entry_wise_mult(one_hots, log_probabilities)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# sum then multiply by -1\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/ops.py:46\u001b[0m, in \u001b[0;36mvector_wise_apply\u001b[0;34m(function, tensor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, \u001b[38;5;28mlist\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must be at least a vector (aka a list of Value objects)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [vector_wise_apply(function, sub_tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m sub_tensor \u001b[38;5;129;01min\u001b[39;00m tensor]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# base case: the final vector dimension\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/ops.py:46\u001b[0m, in \u001b[0;36mvector_wise_apply\u001b[0;34m(function, tensor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, \u001b[38;5;28mlist\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must be at least a vector (aka a list of Value objects)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [vector_wise_apply(function, sub_tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m sub_tensor \u001b[38;5;129;01min\u001b[39;00m tensor]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# base case: the final vector dimension\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/ops.py:48\u001b[0m, in \u001b[0;36mvector_wise_apply\u001b[0;34m(function, tensor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [vector_wise_apply(function, sub_tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m sub_tensor \u001b[38;5;129;01min\u001b[39;00m tensor]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# base case: the final vector dimension\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/ops.py:335\u001b[0m, in \u001b[0;36mlog\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vec, \u001b[38;5;28mlist\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvec should be a list of Value objects\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, Value) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vec), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll elements in vec must be Value objects\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mlog() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vec]\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/engine.py:75\u001b[0m, in \u001b[0;36mValue.log\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 75\u001b[0m     out \u001b[38;5;241m=\u001b[39m Value(math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata), (\u001b[38;5;28mself\u001b[39m,))\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m():\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;241m*\u001b[39m out\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;66;03m# local derivative of ln(x) is 1/x\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(train_iterations):\n",
    "        \n",
    "    train_batch_data = train_dataset[i:i + batch_size]\n",
    "    train_input_toks, train_target_toks = [], []\n",
    "    for entry in train_batch_data:\n",
    "        train_toks_seq = tokenize(entry)\n",
    "        train_input_toks.append(train_toks_seq[:-1])\n",
    "        train_target_toks.append(train_toks_seq[1:])\n",
    "\n",
    "    probabilities, train_loss = model(train_input_toks, train_target_toks)\n",
    "        \n",
    "    if i % val_frequency == 0:\n",
    "        val_batch_data = val_dataset[i % val_frequency:(i % val_frequency) + batch_size]\n",
    "        val_input_toks, val_target_toks = [], []\n",
    "        for entry in val_batch_data:\n",
    "            val_toks_seq = tokenize(entry)\n",
    "            val_input_toks.append(val_toks_seq[:-1])\n",
    "            val_target_toks.append(val_toks_seq[1:])\n",
    "            \n",
    "        probabilities, val_loss = model(val_input_toks, val_target_toks)\n",
    "        print(f'step {i} train loss: {train_loss.data} val loss: {val_loss.data} time: {time.time() - start_time}')\n",
    "\n",
    "    ## backward pass\n",
    "    #set params to 0\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "    # clac gradients\n",
    "    train_loss.backward()\n",
    "    # performing a step of SGD\n",
    "    for p in model.parameters():\n",
    "        p.data -= eta * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
