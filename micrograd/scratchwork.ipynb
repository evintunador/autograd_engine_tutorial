{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r\n",
    "\n",
    "from engine import Value\n",
    "from modules import *\n",
    "from ops import *\n",
    "from gpt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 2\n",
    "        self.vocab_len = 10\n",
    "        self.model_dim = 8\n",
    "        self.max_seq_len = 5\n",
    "        self.seq_len = 3\n",
    "        self.num_heads = 2\n",
    "        self.head_dim = self.model_dim // self.num_heads\n",
    "        self.mlp_mult = 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(Module):\n",
    "    def __init__(self, config):\n",
    "        self.vocab_len = config.vocab_len\n",
    "        self.model_dim = config.model_dim\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.seq_len = config.seq_len\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.mlp_mult = config.mlp_mult\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.num_layers = config.num_layers\n",
    "\n",
    "        self.tok_embeddings = Embedding(self.vocab_len, self.model_dim)\n",
    "        self.scale = self.model_dim ** -0.5\n",
    "        self.pos_embeddings = Embedding(self.max_seq_len, self.model_dim)\n",
    "\n",
    "        self.mask = Mask(self.max_seq_len)\n",
    "        \n",
    "        self.layers = [ResidualLayer(self.model_dim, self.num_heads, self.head_dim, self.max_seq_len, self.mlp_mult, self.mask) \n",
    "                       for _ in range(config.num_layers)]\n",
    "\n",
    "        self.output_proj = Linear(self.model_dim, self.vocab_len)\n",
    "\n",
    "        self.criterion = CrossEntropyLoss(self.vocab_len, pad_token = self.vocab_len - 1)\n",
    "\n",
    "    def __call__(self, input_token_ids, target_token_ids = None):\n",
    "        input_shape = get_shape(input_token_ids)\n",
    "        if len(input_shape) == 1: # if only one sequence is passed in, aka batch_size==1\n",
    "            input_shape = [1] + input_shape\n",
    "            input_tokens = [input_token_ids]\n",
    "\n",
    "        if target_token_ids: # if training\n",
    "            assert input_shape == get_shape(target_token_ids)\n",
    "            target_shape = get_shape(target_token_ids)\n",
    "            assert input_shape[1] == self.max_seq_len\n",
    "            dropout_rate = self.dropout_rate\n",
    "        else: # if inference\n",
    "            target_shape = None\n",
    "            assert input_shape[1] <= self.max_seq_len\n",
    "            dropout_rate = 0.\n",
    "\n",
    "        x = vector_wise_apply(self.tok_embeddings, input_token_ids)\n",
    "        pos = vector_wise_apply(self.pos_embeddings, [list(range(input_shape[1])) for _ in range(input_shape[0])])\n",
    "        x = entry_wise_add(x, pos)\n",
    "        x = vector_wise_apply(mult_vec_by_scalar, x, self.scale)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, dropout_rate)\n",
    "\n",
    "        logits = vector_wise_apply(self.output_proj, vector_wise_apply(layer_norm, x))\n",
    "        probabilities = vector_wise_apply(softmax, logits)\n",
    "\n",
    "        loss = self.criterion(probabilities, target_token_ids) if target_token_ids else None\n",
    "        \n",
    "        return probabilities, loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "math range error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m input_token_ids \u001b[38;5;241m=\u001b[39m [[r\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, config\u001b[38;5;241m.\u001b[39mvocab_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mmax_seq_len)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mbatch_size)]\n\u001b[1;32m      4\u001b[0m target_token_ids \u001b[38;5;241m=\u001b[39m [[r\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, config\u001b[38;5;241m.\u001b[39mvocab_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mmax_seq_len)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mbatch_size)]\n\u001b[0;32m----> 5\u001b[0m probabilities, loss \u001b[38;5;241m=\u001b[39m gpt(input_token_ids, target_token_ids)\n\u001b[1;32m      6\u001b[0m pretty_tensor_print(probabilities)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mGPT.__call__\u001b[0;34m(self, input_token_ids, target_token_ids)\u001b[0m\n\u001b[1;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, dropout_rate)\n\u001b[1;32m     50\u001b[0m logits \u001b[38;5;241m=\u001b[39m vector_wise_apply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj, vector_wise_apply(layer_norm, x))\n\u001b[0;32m---> 51\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m vector_wise_apply(softmax, logits)\n\u001b[1;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(probabilities, target_token_ids) \u001b[38;5;28;01mif\u001b[39;00m target_token_ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probabilities, loss\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/ops.py:46\u001b[0m, in \u001b[0;36mvector_wise_apply\u001b[0;34m(function, tensor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, \u001b[38;5;28mlist\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must be at least a vector (aka a list of Value objects)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [vector_wise_apply(function, sub_tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m sub_tensor \u001b[38;5;129;01min\u001b[39;00m tensor]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# base case: the final vector dimension\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/ops.py:46\u001b[0m, in \u001b[0;36mvector_wise_apply\u001b[0;34m(function, tensor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, \u001b[38;5;28mlist\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must be at least a vector (aka a list of Value objects)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [vector_wise_apply(function, sub_tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m sub_tensor \u001b[38;5;129;01min\u001b[39;00m tensor]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# base case: the final vector dimension\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/ops.py:48\u001b[0m, in \u001b[0;36mvector_wise_apply\u001b[0;34m(function, tensor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [vector_wise_apply(function, sub_tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m sub_tensor \u001b[38;5;129;01min\u001b[39;00m tensor]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# base case: the final vector dimension\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/ops.py:320\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, Value) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vec), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll elements in vec must be Value objects\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# perform entry-wise exponentiation\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m vec_exp \u001b[38;5;241m=\u001b[39m exp(vec)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# calculate the sum of the newly exponentiated vector\u001b[39;00m\n\u001b[1;32m    322\u001b[0m sum_vec_exp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(vec_exp)\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/ops.py:311\u001b[0m, in \u001b[0;36mexp\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vec, \u001b[38;5;28mlist\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvec should be a list of Value objects\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, Value) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vec), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll elements in vec must be Value objects\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mexp() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vec]\n",
      "File \u001b[0;32m~/repos/autograd_engine_tutorial/micrograd/engine.py:66\u001b[0m, in \u001b[0;36mValue.exp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexp\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     out \u001b[38;5;241m=\u001b[39m Value(math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata), (\u001b[38;5;28mself\u001b[39m,))\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m():\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m out\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;66;03m# local derivative of e^x is just e^x, aka out.data\u001b[39;00m\n",
      "\u001b[0;31mOverflowError\u001b[0m: math range error"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "gpt = GPT(config)\n",
    "input_token_ids = [[r.randint(0, config.vocab_len - 1) for _ in range(config.max_seq_len)] for _ in range(config.batch_size)]\n",
    "target_token_ids = [[r.randint(0, config.vocab_len - 1) for _ in range(config.max_seq_len)] for _ in range(config.batch_size)]\n",
    "probabilities, loss = gpt(input_token_ids, target_token_ids)\n",
    "pretty_tensor_print(probabilities)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make Embedding module also do unembedding w/ shared weights? \n",
    "# wouldn't be exactly faithful to pytorch implementation but i'd like to use gradient accumulation & save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
