{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r\n",
    "\n",
    "from engine import Value\n",
    "from modules import *\n",
    "from ops import *\n",
    "from gpt import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addition\n",
    "should we do modular addition instead??? problem with that is we'd have to use a huge modulo to have a reasonably sized training dataset right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_digits = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(k: int):\n",
    "    '''\n",
    "    creates a string representation of an arithmetic addition operation where the digits are in reverse-order\n",
    "    ex:\n",
    "    123 + 45 = 168 would be '321+54=861'\n",
    "    this reversal is meant to make the task easier on teh transformer\n",
    "    '''\n",
    "    max_num = int('9' * k)\n",
    "    num1 = r.randint(0, max_num)\n",
    "    num2 = r.randint(0, max_num)\n",
    "    num3 = num1 + num2\n",
    "    return f'b{str(num1)[::-1]}+{str(num2)[::-1]}={str(num3)[::-1]}e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = set()\n",
    "max_possible_dataset_size = int('9' * max_digits)**2\n",
    "dataset_size = 2\n",
    "i = 0\n",
    "while dataset_size < max_possible_dataset_size:\n",
    "    i += 1\n",
    "    dataset_size = 2 ** i\n",
    "dataset_size = 2 ** (i - 1)\n",
    "assert dataset_size < max_possible_dataset_size\n",
    "while len(dataset) < dataset_size:\n",
    "    dataset.add(generate_data(max_digits))\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7782\n",
      "410\n"
     ]
    }
   ],
   "source": [
    "split_size = int(0.95 * len(dataset))\n",
    "dataset = list(dataset)\n",
    "train_dataset = dataset[:split_size]\n",
    "val_dataset = dataset[split_size:]\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b89+86=661e\n",
      "[12, 8, 9, 10, 8, 6, 11, 6, 6, 1, 13]\n",
      "[12, 8, 9, 10, 8, 6, 11, 6, 6, 1] [8, 9, 10, 8, 6, 11, 6, 6, 1, 13]\n",
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: '+', 11: '=', 12: 'b', 13: 'e', 14: 'p'}\n",
      "98+68=166\n"
     ]
    }
   ],
   "source": [
    "tokenizer = {\n",
    "    '0':0, '1':1, '2':2, '3':3, '4':4, '5':5, '6':6, '7':7, '8':8, '9':9, \n",
    "    '+':10, '=':11, \n",
    "    'b':12, # beginning of sequence token\n",
    "    'e':13, # end of sequence token\n",
    "    'p':14 # padding token\n",
    "}\n",
    "v = len(tokenizer)\n",
    "max_data_len = 4 + max_digits * 3 + 1\n",
    "\n",
    "def tokenize(equation):\n",
    "    out = []\n",
    "    for c in equation:\n",
    "        out.append(tokenizer[c])\n",
    "    while len(out) < max_data_len:\n",
    "        out.append(tokenizer['p'])\n",
    "    return out\n",
    "\n",
    "print(train_dataset[0])\n",
    "tokens = tokenize(train_dataset[0])\n",
    "print(tokens)\n",
    "\n",
    "input_tokens, target_tokens = tokens[:-1], tokens[1:]\n",
    "print(input_tokens, target_tokens)\n",
    "\n",
    "reverse_tokenizer = {val:tok for tok, val in zip(tokenizer, tokenizer.values())}\n",
    "print(reverse_tokenizer)\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    char_list = [reverse_tokenizer[t] for t in tokens]\n",
    "    eos_idx = char_list.index('e')\n",
    "    chars = \"\".join(char_list[1:eos_idx])\n",
    "    num1, temp = chars.split('+')\n",
    "    num2, num3 = temp.split('=')\n",
    "    return f'{num1[::-1]}+{num2[::-1]}={num3[::-1]}'\n",
    "    \n",
    "output = decode_tokens(tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486 25 19\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_iterations = split_size // batch_size\n",
    "val_iterations = (dataset_size - split_size) // batch_size\n",
    "val_frequency = train_iterations // val_iterations\n",
    "print(train_iterations, val_iterations, val_frequency)\n",
    "\n",
    "config = {\n",
    "    'vocab_len':v,\n",
    "    'model_dim':4,\n",
    "    'max_seq_len':max_data_len - 1,\n",
    "    'num_heads':2,\n",
    "    'head_dim':2,\n",
    "    'mlp_mult':2,\n",
    "    'dropout_rate':0.1,\n",
    "    'num_layers':1\n",
    "}\n",
    "model = GPT(config)\n",
    "\n",
    "eta = -0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss: Value(data=433.288, grad=0.000)\n",
      "step 19 loss: Value(data=433.288, grad=0.000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(val_frequency + 1):#train_iterations):\n",
    "        \n",
    "    if i % val_frequency == 0:\n",
    "        val_batch_data = val_dataset[i % val_frequency:(i % val_frequency) + batch_size]\n",
    "        val_input_toks, val_target_toks = [], []\n",
    "        for entry in val_batch_data:\n",
    "            val_toks_seq = tokenize(entry)\n",
    "            val_input_toks.append(val_toks_seq[:-1])\n",
    "            val_target_toks.append(val_toks_seq[1:])\n",
    "            \n",
    "        probabilities, loss = model(val_input_toks, val_target_toks)\n",
    "        print(f'step {i} loss: {loss}')\n",
    "        \n",
    "    train_batch_data = train_dataset[i:i + batch_size]\n",
    "    train_input_toks, train_target_toks = [], []\n",
    "    for entry in train_batch_data:\n",
    "        train_toks_seq = tokenize(entry)\n",
    "        train_input_toks.append(train_toks_seq[:-1])\n",
    "        train_target_toks.append(train_toks_seq[1:])\n",
    "\n",
    "    probabilities, loss = model(train_input_toks, train_target_toks)\n",
    "\n",
    "    ## backward pass\n",
    "    #set params to 0\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "    # clac gradients\n",
    "    loss.backward()\n",
    "    # performing a step of SGD\n",
    "    for p in model.parameters():\n",
    "        p.data += -eta * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [Value(data=-0.028, grad=0.000), Value(data=0.016, grad=0.000), Value(data=0.034, grad=0.000), Value(data=-0.006, grad=0.000)]\n",
      "  [Value(data=0.025, grad=0.000), Value(data=-0.002, grad=0.000), Value(data=0.010, grad=0.000), Value(data=0.001, grad=0.000)]\n",
      "  [Value(data=-0.016, grad=0.000), Value(data=-0.026, grad=0.000), Value(data=-0.017, grad=0.000), Value(data=0.027, grad=0.000)]\n",
      "]\n",
      "[Value(data=-0.028, grad=0.000), Value(data=0.016, grad=0.000), Value(data=0.034, grad=0.000), Value(data=-0.006, grad=0.000), Value(data=0.025, grad=0.000), Value(data=-0.002, grad=0.000), Value(data=0.010, grad=0.000), Value(data=0.001, grad=0.000), Value(data=-0.016, grad=0.000), Value(data=-0.026, grad=0.000), Value(data=-0.017, grad=0.000), Value(data=0.027, grad=0.000)]\n"
     ]
    }
   ],
   "source": [
    "e = Embedding(3, 4)\n",
    "pretty_tensor_print(e.weight)\n",
    "print(e.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=-0.017, grad=0.000), Value(data=0.013, grad=0.000), Value(data=-0.032, grad=0.000), Value(data=-0.007, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.004, grad=0.000), Value(data=0.010, grad=0.000), Value(data=0.013, grad=0.000), Value(data=-0.003, grad=0.000), Value(data=0.000, grad=0.000), Value(data=-0.045, grad=0.000), Value(data=0.025, grad=0.000), Value(data=0.008, grad=0.000), Value(data=-0.043, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.022, grad=0.000), Value(data=-0.016, grad=0.000), Value(data=-0.006, grad=0.000), Value(data=-0.019, grad=0.000), Value(data=0.000, grad=0.000), Value(data=-0.012, grad=0.000), Value(data=-0.029, grad=0.000), Value(data=-0.002, grad=0.000), Value(data=-0.004, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.006, grad=0.000), Value(data=-0.025, grad=0.000), Value(data=0.003, grad=0.000), Value(data=-0.007, grad=0.000), Value(data=0.000, grad=0.000), Value(data=-0.044, grad=0.000), Value(data=-0.014, grad=0.000), Value(data=-0.003, grad=0.000), Value(data=0.031, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.016, grad=0.000), Value(data=0.007, grad=0.000), Value(data=-0.017, grad=0.000), Value(data=0.024, grad=0.000), Value(data=0.000, grad=0.000), Value(data=-0.002, grad=0.000), Value(data=0.001, grad=0.000), Value(data=0.017, grad=0.000), Value(data=0.013, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.004, grad=0.000), Value(data=0.022, grad=0.000), Value(data=0.018, grad=0.000), Value(data=-0.018, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.010, grad=0.000), Value(data=-0.032, grad=0.000), Value(data=-0.005, grad=0.000), Value(data=0.004, grad=0.000), Value(data=0.000, grad=0.000), Value(data=-0.020, grad=0.000), Value(data=0.001, grad=0.000), Value(data=-0.002, grad=0.000), Value(data=0.003, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.030, grad=0.000), Value(data=0.010, grad=0.000), Value(data=-0.006, grad=0.000), Value(data=-0.005, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.029, grad=0.000), Value(data=0.009, grad=0.000), Value(data=0.017, grad=0.000), Value(data=-0.020, grad=0.000), Value(data=0.000, grad=0.000), Value(data=-0.015, grad=0.000), Value(data=0.004, grad=0.000), Value(data=-0.021, grad=0.000), Value(data=-0.018, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.002, grad=0.000), Value(data=-0.006, grad=0.000), Value(data=-0.002, grad=0.000), Value(data=0.034, grad=0.000), Value(data=0.000, grad=0.000)]\n"
     ]
    }
   ],
   "source": [
    "mask = Mask(5)\n",
    "mhsa = MultiHeadSelfAttention(4, 2, 2, 5, mask)\n",
    "print(mhsa.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=-0.003, grad=0.000), Value(data=-0.018, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.010, grad=0.000), Value(data=-0.029, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.006, grad=0.000), Value(data=-0.007, grad=0.000), Value(data=0.000, grad=0.000), Value(data=-0.024, grad=0.000), Value(data=-0.025, grad=0.000), Value(data=0.000, grad=0.000), Value(data=-0.024, grad=0.000), Value(data=-0.030, grad=0.000), Value(data=-0.007, grad=0.000), Value(data=0.038, grad=0.000), Value(data=0.000, grad=0.000), Value(data=0.022, grad=0.000), Value(data=0.006, grad=0.000), Value(data=0.004, grad=0.000), Value(data=0.008, grad=0.000), Value(data=0.000, grad=0.000)]\n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptron(2,4,2)\n",
    "print(mlp.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
