{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r\n",
    "\n",
    "from engine import Value\n",
    "from modules import *\n",
    "from ops import *\n",
    "from gpt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(vec):\n",
    "    '''\n",
    "    performs the softmax operation across the input vector, giving us a list of probaiblities\n",
    "    '''\n",
    "    assert isinstance(vec, list), \"vec should be a list of Value objects\"\n",
    "    assert all(isinstance(x, Value) for x in vec), \"All elements in vec must be Value objects\"\n",
    "    # Subtract the max value from each element for numerical stability\n",
    "    max_val = max(x.data for x in vec)\n",
    "    vec_shifted = [x - max_val for x in vec]\n",
    "    # perform entry-wise exponentiation\n",
    "    vec_exp = exp(vec_shifted)\n",
    "    # calculate the sum of the newly exponentiated vector\n",
    "    sum_vec_exp = sum(vec_exp)\n",
    "    # return a vector of each exponentiated entry divided by the sum\n",
    "    return [x / sum_vec_exp for x in vec_exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 2\n",
    "        self.vocab_len = 10\n",
    "        self.model_dim = 8\n",
    "        self.max_seq_len = 5\n",
    "        self.seq_len = 3\n",
    "        self.num_heads = 2\n",
    "        self.head_dim = self.model_dim // self.num_heads\n",
    "        self.mlp_mult = 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(Module):\n",
    "    def __init__(self, config):\n",
    "        self.vocab_len = config.vocab_len\n",
    "        self.model_dim = config.model_dim\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.seq_len = config.seq_len\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.mlp_mult = config.mlp_mult\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.num_layers = config.num_layers\n",
    "\n",
    "        self.tok_embeddings = Embedding(self.vocab_len, self.model_dim)\n",
    "        self.scale = self.model_dim ** -0.5\n",
    "        self.pos_embeddings = Embedding(self.max_seq_len, self.model_dim)\n",
    "\n",
    "        self.mask = Mask(self.max_seq_len)\n",
    "        \n",
    "        self.layers = [ResidualLayer(self.model_dim, self.num_heads, self.head_dim, self.max_seq_len, self.mlp_mult, self.mask) \n",
    "                       for _ in range(config.num_layers)]\n",
    "\n",
    "        self.output_proj = Linear(self.model_dim, self.vocab_len)\n",
    "\n",
    "        self.criterion = CrossEntropyLoss(self.vocab_len, pad_token = self.vocab_len - 1)\n",
    "\n",
    "    def __call__(self, input_token_ids, target_token_ids = None):\n",
    "        input_shape = get_shape(input_token_ids)\n",
    "        if len(input_shape) == 1: # if only one sequence is passed in, aka batch_size==1\n",
    "            input_shape = [1] + input_shape\n",
    "            input_tokens = [input_token_ids]\n",
    "\n",
    "        if target_token_ids: # if training\n",
    "            assert input_shape == get_shape(target_token_ids)\n",
    "            target_shape = get_shape(target_token_ids)\n",
    "            assert input_shape[1] == self.max_seq_len\n",
    "            dropout_rate = self.dropout_rate\n",
    "        else: # if inference\n",
    "            target_shape = None\n",
    "            assert input_shape[1] <= self.max_seq_len\n",
    "            dropout_rate = 0.\n",
    "\n",
    "        x = vector_wise_apply(self.tok_embeddings, input_token_ids)\n",
    "        pos = vector_wise_apply(self.pos_embeddings, [list(range(input_shape[1])) for _ in range(input_shape[0])])\n",
    "        x = entry_wise_add(x, pos)\n",
    "        x = vector_wise_apply(mult_vec_by_scalar, x, self.scale)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, dropout_rate)\n",
    "\n",
    "        logits = vector_wise_apply(self.output_proj, vector_wise_apply(layer_norm, x))\n",
    "        probabilities = vector_wise_apply(softmax, logits)\n",
    "        pretty_tensor_print(probabilities)\n",
    "        loss = self.criterion(probabilities, target_token_ids) if target_token_ids else None\n",
    "        \n",
    "        return probabilities, loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "gpt = GPT(config)\n",
    "input_token_ids = [[r.randint(0, config.vocab_len - 1) for _ in range(config.max_seq_len)] for _ in range(config.batch_size)]\n",
    "target_token_ids = [[r.randint(0, config.vocab_len - 1) for _ in range(config.max_seq_len)] for _ in range(config.batch_size)]\n",
    "probabilities, loss = gpt(input_token_ids, target_token_ids)\n",
    "pretty_tensor_print(probabilities)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make Embedding module also do unembedding w/ shared weights? \n",
    "# wouldn't be exactly faithful to pytorch implementation but i'd like to use gradient accumulation & save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
